{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kaggle比赛情感分析题目：Bag of Words Meets Bags of Popcorn\n",
    "* Kaggle比赛地址：https://www.kaggle.com/c/word2vec-nlp-tutorial#description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据特征处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、读取并查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "['id' 'sentiment' 'review']\n",
      "(50000, 2)\n",
      "['id' 'review']\n",
      "(25000, 2)\n",
      "['id' 'review']\n",
      "       id  sentiment                                             review\n",
      "0  5814_8          1  With all this stuff going down at the moment w...\n",
      "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
      "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
      "          id                                             review\n",
      "0   \"9999_0\"  \"Watching Time Chasers, it obvious that it was...\n",
      "1  \"45057_0\"  \"I saw this film about 20 years ago and rememb...\n",
      "2  \"15561_0\"  \"Minor Spoilers<br /><br />In New York, Joan B...\n",
      "         id                                             review\n",
      "0  12311_10  Naturally in a film who's main themes are of m...\n",
      "1    8348_2  This movie is a disaster within a disaster fil...\n",
      "2    5828_4  All in all, this is a movie for kids. We saw i...\n"
     ]
    }
   ],
   "source": [
    "root_dir = \"data\"\n",
    "# 载入数据集\n",
    "train = pd.read_csv('%s/%s' % (root_dir, 'labeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=1)\n",
    "unlabel_train = pd.read_csv('%s/%s' % (root_dir, 'unlabeledTrainData.tsv'), header=0, delimiter=\"\\t\", quoting=3)   \n",
    "test = pd.read_csv('%s/%s' % (root_dir, 'testData.tsv'), header=0, delimiter=\"\\t\", quoting=1)\n",
    "\n",
    "print(train.shape)\n",
    "print(train.columns.values)\n",
    "print(unlabel_train.shape)\n",
    "print(unlabel_train.columns.values)\n",
    "print(test.shape)\n",
    "print(test.columns.values)\n",
    "\n",
    "print(train.head(3))\n",
    "print(unlabel_train.head(3))\n",
    "print(test.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.样本分布（相当的均衡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.sentiment==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12500, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.sentiment==0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从原始数据中可以看出：\n",
    "* 1.labeledTrainData数据用于模型训练；unlabeledTrainData数据用于Word2vec提取特征；testData数据用于提交结果预测。\n",
    "* 2.文本数据来自网络爬虫数据，带有html格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.去除HTML标签+数字+全部小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stuff going moment mj started listening music watching odd documentary watched wiz watched moonwalker maybe want get certain insight guy thought really cool eighties maybe make mind whether guilty innocent moonwalker part biography part feature film remember going see cinema originally released subtle messages mj feeling towards press also obvious message drugs bad kay visually impressive course michael jackson unless remotely like mj anyway going hate find boring may call mj egotist consenting making movie mj fans would say made fans true really nice actual feature film bit finally starts minutes excluding smooth criminal sequence joe pesci convincing psychopathic powerful drug lord wants mj dead bad beyond mj overheard plans nah joe pesci character ranted wanted people know supplying drugs etc dunno maybe hates mj music lots cool things like mj turning car robot whole speed demon sequence also director must patience saint came filming kiddy bad sequence usually directors hate working one kid let alone whole bunch performing complex dance scene bottom line movie people like mj one level another think people stay away try give wholesome message ironically mj bestest buddy movie girl michael jackson truly one talented people ever grace planet guilty well attention gave subject hmmm well know people different behind closed doors know fact either extremely nice stupid guy one sickest liars hope latter \n",
      "\n",
      "watching time chasers obvious made bunch friends maybe sitting around one day film school said hey let pool money together make really bad movie something like ever said still ended making really bad movie dull story bad script lame acting poor cinematography bottom barrel stock music etc corners cut except one would prevented film release life like \n",
      "\n",
      "naturally film main themes mortality nostalgia loss innocence perhaps surprising rated highly older viewers younger ones however craftsmanship completeness film anyone enjoy pace steady constant characters full engaging relationships interactions natural showing need floods tears show emotion screams show fear shouting show dispute violence show anger naturally joyce short story lends film ready made structure perfect polished diamond small changes huston makes inclusion poem fit neatly truly masterpiece tact subtlety overwhelming beauty\n"
     ]
    }
   ],
   "source": [
    "def review_to_wordlist(review):\n",
    "    '''\n",
    "    把IMDB的评论转成词序列\n",
    "    '''\n",
    "    from bs4 import BeautifulSoup\n",
    "    # 1.去掉HTML标签，拿到内容\n",
    "    review_text = BeautifulSoup(review, \"html.parser\").get_text()\n",
    "    \n",
    "    import re\n",
    "    # 用正则表达式取出符合规范的部分\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    \n",
    "    # 小写化所有的词，并转成词list\n",
    "    words_list = review_text.lower().split()\n",
    "    \n",
    "    #去除停用词。需要下载nltk库，并且下载stopwords。\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwords = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words_list if word not in stopwords]\n",
    "    \n",
    "    # 返回words\n",
    "    return words\n",
    "\n",
    "# 预处理数据\n",
    "label = train['sentiment']\n",
    "train_data = []\n",
    "for i in range(len(train['review'])):\n",
    "    train_data.append(' '.join(review_to_wordlist(train['review'][i])))\n",
    "    \n",
    "unlable_data = []    \n",
    "for i in range(len(unlabel_train['review'])):\n",
    "    unlable_data.append(' '.join(review_to_wordlist(unlabel_train['review'][i])))   \n",
    "    \n",
    "test_data = []\n",
    "for i in range(len(test['review'])):\n",
    "    test_data.append(' '.join(review_to_wordlist(test['review'][i])))\n",
    "\n",
    "# 预览数据\n",
    "print(train_data[0], '\\n')\n",
    "print(unlable_data[0], '\\n')\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 查看数据量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 50000 25000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data),len(unlable_data),len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、特征工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把文本转换为向量，有几种常见的文本向量处理方法，比如：\n",
    "\n",
    "* 1.单词计数\n",
    "* 2.TF-IDF向量\n",
    "* 3.Word2vec向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Count词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 2 1]\n",
      " [0 1 1 0 1 0 0]]\n",
      "\n",
      "vocabulary list:\n",
      "\n",
      " {'sed': 5, 'about': 0, 'the': 6, 'lack': 3, 'of': 4, 'any': 2, 'actually': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "X_test = ['I sed about sed the lack','of any Actually']\n",
    "\n",
    "count_vec=CountVectorizer(stop_words=None)\n",
    "print (count_vec.fit_transform(X_test).toarray())\n",
    "print ('\\nvocabulary list:\\n\\n',count_vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count处理结束.\n",
      "train: \n",
      " (1, 4000)\n",
      "test: \n",
      " (1, 4000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vec = CountVectorizer(\n",
    "    max_features=4000,#过滤掉低频\n",
    "    analyzer='word', # tokenise by character ngrams\n",
    "    ngram_range=(1,2),  # 二元n-gram模型\n",
    "    stop_words = 'english')\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "count_vec.fit(data_all)\n",
    "data_all = count_vec.transform(data_all)\n",
    "\n",
    "# 恢复成训练集和测试集部分\n",
    "count_train_x = data_all[:len_train]\n",
    "count_test_x = data_all[len_train:]\n",
    "\n",
    "print('count处理结束.')\n",
    "\n",
    "print(\"train: \\n\", np.shape(count_train_x[0]))\n",
    "print(\"test: \\n\", np.shape(count_test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count特征\n",
    "* count_train_x\n",
    "* count_test_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.TF-IDF词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF处理结束.\n",
      "train: \n",
      " (1, 4000)\n",
      "test: \n",
      " (1, 4000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer as TFIDF\n",
    "\"\"\"\n",
    "min_df: 最小支持度为2（词汇出现的最小次数）\n",
    "max_features: 默认为None，可设为int，对所有关键词的term frequency进行降序排序，只取前max_features个作为关键词集\n",
    "strip_accents: 将使用ascii或unicode编码在预处理步骤去除raw document中的重音符号\n",
    "analyzer: 设置返回类型\n",
    "token_pattern: 表示token的正则表达式，需要设置analyzer == 'word'，默认的正则表达式选择2个及以上的字母或数字作为token，标点符号默认当作token分隔符，而不会被当作token\n",
    "ngram_range: 词组切分的长度范围\n",
    "use_idf: 启用逆文档频率重新加权\n",
    "use_idf：默认为True，权值是tf*idf，如果设为False，将不使用idf，就是只使用tf，相当于CountVectorizer了。\n",
    "smooth_idf: idf平滑参数，默认为True，idf=ln((文档总数+1)/(包含该词的文档数+1))+1，如果设为False，idf=ln(文档总数/包含该词的文档数)+1\n",
    "sublinear_tf: 默认为False，如果设为True，则替换tf为1 + log(tf)\n",
    "stop_words: 设置停用词，设为english将使用内置的英语停用词，设为一个list可自定义停用词，设为None不使用停用词，设为None且max_df∈[0.7, 1.0)将自动根据当前的语料库建立停用词表\n",
    "\"\"\"\n",
    "tfidf = TFIDF(min_df=2,\n",
    "           max_features=4000,#过滤掉低频\n",
    "           strip_accents='unicode',\n",
    "           analyzer='word',\n",
    "           token_pattern=r'\\w{1,}',\n",
    "           ngram_range=(1,2),  # 二元n-gram模型\n",
    "           use_idf=1,\n",
    "           smooth_idf=1,\n",
    "           sublinear_tf=1,\n",
    "           stop_words = 'english') # 去掉英文停用词\n",
    "\n",
    "# 合并训练和测试集以便进行TFIDF向量化操作\n",
    "data_all = train_data + test_data\n",
    "len_train = len(train_data)\n",
    "\n",
    "tfidf.fit(data_all)\n",
    "data_all = tfidf.transform(data_all)\n",
    "# 恢复成训练集和测试集部分\n",
    "tfidf_train_x = data_all[:len_train]\n",
    "tfidf_test_x = data_all[len_train:]\n",
    "print('TF-IDF处理结束.')\n",
    "\n",
    "print(\"train: \\n\", np.shape(tfidf_train_x[0]))\n",
    "print(\"test: \\n\", np.shape(tfidf_test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 4000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Word2vec词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* gensim.models.word2vec.Word2Vec 输入数据是字符的list格式，所以需要对数据进行预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 输入数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预处理训练数据\n",
    "train_words = []\n",
    "for i in train_data:\n",
    "    train_words.append(i.split())\n",
    "    \n",
    "#预处理特征数据\n",
    "unlable_words = []\n",
    "for i in unlable_data:\n",
    "    unlable_words.append(i.split())\n",
    "\n",
    "#预处理测试数据\n",
    "test_words = []\n",
    "for i in test_data:\n",
    "    test_words.append(i.split())\n",
    "\n",
    "#合并数据\n",
    "all_words = train_words + unlable_words + test_words\n",
    "\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'get', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'whether', 'guilty', 'innocent', 'moonwalker', 'part', 'biography', 'part', 'feature', 'film', 'remember', 'going', 'see', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'towards', 'press', 'also', 'obvious', 'message', 'drugs', 'bad', 'kay', 'visually', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'anyway', 'going', 'hate', 'find', 'boring', 'may', 'call', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'would', 'say', 'made', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'beyond', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'etc', 'dunno', 'maybe', 'hates', 'mj', 'music', 'lots', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'whole', 'speed', 'demon', 'sequence', 'also', 'director', 'must', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'one', 'kid', 'let', 'alone', 'whole', 'bunch', 'performing', 'complex', 'dance', 'scene', 'bottom', 'line', 'movie', 'people', 'like', 'mj', 'one', 'level', 'another', 'think', 'people', 'stay', 'away', 'try', 'give', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'one', 'talented', 'people', 'ever', 'grace', 'planet', 'guilty', 'well', 'attention', 'gave', 'subject', 'hmmm', 'well', 'know', 'people', 'different', 'behind', 'closed', 'doors', 'know', 'fact', 'either', 'extremely', 'nice', 'stupid', 'guy', 'one', 'sickest', 'liars', 'hope', 'latter']\n"
     ]
    }
   ],
   "source": [
    "# 预览数据\n",
    "print(all_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Word2vec模型训练保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xuliang\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import os\n",
    "\n",
    "# 设定词向量训练的参数\n",
    "size = 100      # Word vector dimensionality\n",
    "min_count = 3   # Minimum word count\n",
    "num_workers = 4 # Number of threads to run in parallel\n",
    "window = 10     # Context window size\n",
    "model_name = '{}size_{}min_count_{}window.model'.format(size, min_count, window)\n",
    "\n",
    "wv_model = Word2Vec(all_words, workers=num_workers, size=size, min_count = min_count,window = window)\n",
    "wv_model.init_sims(replace=True)#模型训练好后，锁定模型\n",
    "\n",
    "wv_model.save(model_name)#保存模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Word2vec模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "wv_model = Word2Vec.load(\"100size_3min_count_10window.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 word2vec特征处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处画风比较奇特:\n",
    "* 将一个句子对应的词向量求和取平均，做为机器学习的特征，但是效果还不错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def to_review_vector(review):\n",
    "    global word_vec\n",
    "    word_vec = np.zeros((1,100))\n",
    "    for word in review:\n",
    "        if word in wv_model:\n",
    "            word_vec += np.array([wv_model[word]])\n",
    "    return pd.Series(word_vec.mean(axis = 0))\n",
    "\n",
    "train_data_features = []\n",
    "\n",
    "for i in train_words:\n",
    "    train_data_features.append(to_review_vector(i))\n",
    "\n",
    "test_data_features = []\n",
    "for i in test_words:\n",
    "    test_data_features.append(to_review_vector(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、机器学习建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TF-IDF+朴素贝叶斯模型+交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "多项式贝叶斯分类器5折交叉验证得分:  [0.92969136 0.93232992 0.93167488 0.93575024 0.92634304]\n",
      "多项式贝叶斯分类器5折交叉验证得分:  0.9311578880000001\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "# 朴素贝叶斯训练\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "\n",
    "model_MNB = MNB() # (alpha=1.0, class_prior=None, fit_prior=True)\n",
    "# 为了在预测的时候使用\n",
    "model_MNB.fit(tfidf_train_x, label)\n",
    "\n",
    "print(\"多项式贝叶斯分类器5折交叉验证得分: \", cross_val_score(model_MNB, tfidf_train_x, label, cv=5, scoring='roc_auc'))\n",
    "print(\"多项式贝叶斯分类器5折交叉验证得分: \", np.mean(cross_val_score(model_MNB, tfidf_train_x, label, cv=5, scoring='roc_auc')))\n",
    "\n",
    "\n",
    "test_predicted = np.array(model_MNB.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test['id'].values, 'sentiment': test_predicted})\n",
    "print(submission_df.head())\n",
    "submission_df.to_csv('submission_mnb_tfidf.csv', index = False)\n",
    "\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. TF-IDF+逻辑回归模型+网格搜索交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最好的参数：\n",
      "{'C': 1}\n",
      "最好的得分：\n",
      "0.9507756479999999\n",
      "网格搜索参数及得分：\n",
      "[mean: 0.93949, std: 0.00324, params: {'C': 0.1}, mean: 0.95078, std: 0.00276, params: {'C': 1}, mean: 0.94398, std: 0.00275, params: {'C': 10}]\n",
      "网格搜索结果：\n",
      "{'mean_fit_time': array([0.12460012, 0.17419996, 0.4506001 ]), 'std_fit_time': array([0.01323034, 0.02166458, 0.01783922]), 'mean_score_time': array([0.0026    , 0.00319996, 0.00260005]), 'std_score_time': array([0.00048982, 0.00040011, 0.00048996]), 'param_C': masked_array(data=[0.1, 1, 10],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'C': 0.1}, {'C': 1}, {'C': 10}], 'split0_test_score': array([0.93795456, 0.95020352, 0.9436768 ]), 'split1_test_score': array([0.94196192, 0.9542512 , 0.94876688]), 'split2_test_score': array([0.93759728, 0.9471152 , 0.9402704 ]), 'split3_test_score': array([0.9444336 , 0.95359056, 0.94413712]), 'split4_test_score': array([0.93548752, 0.94871776, 0.94303536]), 'mean_test_score': array([0.93948698, 0.95077565, 0.94397731]), 'std_test_score': array([0.00324066, 0.00275551, 0.00274533]), 'rank_test_score': array([3, 1, 2]), 'split0_train_score': array([0.9499337 , 0.97234608, 0.9860402 ]), 'split1_train_score': array([0.94941395, 0.97180812, 0.98526184]), 'split2_train_score': array([0.95103983, 0.97310123, 0.98656722]), 'split3_train_score': array([0.94978255, 0.97232675, 0.98596239]), 'split4_train_score': array([0.95038084, 0.97259464, 0.98615951]), 'mean_train_score': array([0.95011017, 0.97243536, 0.98599823]), 'std_train_score': array([0.0005587 , 0.00041999, 0.0004231 ])}\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 设定grid search的参数\n",
    "grid_values = {'C': [0.1, 1, 10]}  \n",
    "# 设定打分为roc_auc\n",
    "\"\"\"\n",
    "penalty: l1 or l2, 用于指定惩罚中使用的标准。\n",
    "\"\"\"\n",
    "model_LR = GridSearchCV(LR(penalty='l2', dual=True, random_state=0), grid_values, scoring='roc_auc', cv=5)\n",
    "model_LR.fit(tfidf_train_x, label)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最好的参数：\")\n",
    "print( model_LR.best_params_)\n",
    "\n",
    "print(\"最好的得分：\")\n",
    "print(model_LR.best_score_)\n",
    "\n",
    "\n",
    "print(\"网格搜索参数及得分：\")\n",
    "print(model_LR.grid_scores_)\n",
    "\n",
    "print(\"网格搜索结果：\")\n",
    "print(model_LR.cv_results_)\n",
    "\n",
    "model_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "model_LR.fit(tfidf_train_x, label)\n",
    "\n",
    "test_predicted = np.array(model_LR.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_lr_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAt0AAABHCAYAAADbVfv+AAAd5ElEQVR4Ae2dz4sbV7bHv34MpgNDWw7EUWYw1JC30KJ5FoFAmRAQYwh41TU7eZXajZfaGOS/oAXe9NJ5q/LK2rm8agj4IQiDCwJBhl5oMcYFZjKyA2nFDKTxRo9zf5RularUUrvlbttfgadUVfeee+6nqjOnTn3v0bnpdDoFPyRAAiRAAiRAAiRAAiRAAmsj8F9rs0zDJEACJEACJEACJEACJEACigCDbt4IJEACJEACJEACJEACJLBmAgy61wyY5kmABEiABEiABEiABEiAQTfvARIgARIgARIgARIgARJYMwEG3WsGTPMkQAIkQAIkQAIkQAIkwKCb9wAJkAAJkAAJkAAJkAAJrJnAH07a/vj7H/DZk9fYvvIF4m8unqD5FL07T3Ebm3h860v4b2L55T6Cey/wcPNT/PvvW6i/ia0j+q6PxxED8zQJkAAJkAAJkAAJvBMEJkh6IXqJcdbvIur6qC3wfZL0EGYdAL8boeu7PVLEQQdRZiPEbhzAy/aBoo1wN0bgNkhjBJ2ZBYS7iHMNCn7DRzfqIueGMx4z3Q4MfiUBEiABEiABEiABEni7BNJYAm4JimPE8S5CE1BPKtywwbIEybH02Q1N0G57mIBbgndlUwLyCJ0gRmpsFm3shkDUCRDbBibglmBejSHBdNRBkDWwAbf1O8ZumKAX9pBYNwr+v0NBt4furWuYvmmWWwBc2kIsttac5S6w5i4JkAAJkAAJkAAJkIBLYJKgH0mmumWy0B5aXR9IehjYANhtjwlGA0mJh2jarLTXguoyGEHFu+lQZbj9VsNky2vw2yGACENlM8VAsuR+Fy1jw2t1lYoi0g2QDiXD7aPVMNnzmg9tYqgD98kI2o1mlj3XNhIMRuVRd4m85AD9737CjVd2hudx/9uv0b4EwMoyLn+Oadt4uf8jzu29KpGT/Jaz48pNrORi5/rnwJ5IRvRHt/nNyEjkmDM2SuQlZmzrKVy/svb2rCNLsfPIyUuK83baA0j6j3D1+Xncv34R/b0XeGjM7ly/hu6WHWPRtppHZvvby0jvPcXtnF/W5mL/LFPbWrNEdg1cP21b95rYftySAAmQAAmQAAmQwNsiMBkNoELo+kwaUqs3JPLCaDwBvNnxI31KxiroXtRD2ayNMZKQOgvKAdR8dOP4yCGAEZRbC1om0qBEHDOX6U76EnBLsCtZ5S9wf/M1btz7UQFZYH/u1MMnz4G/iY1reHwZePjkJ/T2881u7/2ClspeyzjSZh/Bd+bYt59iG69x48E+xvluek8C571XUIG22Li+CTx/iuD7AwASoFr9t/jwOXbwCle/q7ClAnSZtwTadt6vcPVOcd6vceMfwF3VRmwCt/eKbcqclbkdxUM4P0Vy5YuSDLw8cLj+mflY//Z/zHT0wvvfV84b3hfR/mpTOZT8LFzkc4DBs9cANtE9Uc29Mc8NCZAACZAACZAACSxJYDK2Qm6nQ62uss46eHWOq681NFqyss9mrQGkA6UHz7LlXhOS105s5hsTJJJOR4i2CK4nYxPXikQkQBDofz1HF+I1lYVZ1tpk5BG2tWa71oB2w2S+lRs9JKLrtunzgutzme70N2mxAU8y27iI9t+voV3otMzu9pUtnR2XJ4ktCYhf4fZ+iu6WfRcAbF/5b7Mg8iK8CwBevYb/1df62KU/o735Ag9f/a7S+HOLHV/+rrLN2x9LRwBbX2KaZZxTpJKp3/woe1XRveWhW+X4/i8q2+760/rLeeDJK/S+P8gtCN35yi68vABP4tlXh0hfAr7iVTWAzHUZHhWLT+f889C6/FQxHewD3s+HamD/T3rhav2brzH9xvryCXb2XuH2s39hjIuov/wX+sLm8ieGvW3HLQmQAAmQAAmQAAmcfQI1v4tYFj12gmyhZH4hpYcgjlDvhQgDO5/ZQsrJWPLcomDpoxXFiLt2UWWInl2Q6QWIozp6YYiZCXchZQ1+N1aLMzuBXWy54kJKFWxKVvjOI5yTf/1SQY2dwXLbSx9hW1r+9p/yrHVm5bwJ9rMD1V+2PlGZZsmgKz/v/ID+S9tcglIJiF/gMzMPnQG35/PbcSFolbP1P22oRg9/VU8h+Q5qzzwolJw58lAFDxs0F/uX+ee39VsEkbbUmxcV39t75prZDLgyZFkcYCB8zMPKjvPwUxyP+yRAAiRAAiRAAiRwNgmY7HRnhK4EzLJQMuoCvRBBLzGa7hhBEKLX2NXnY1nkKAspnYWSMjmbtRYxSKOls+smO57GAYKwh4ZdrCkLPGUhpV2MOUnQCwJ0RnaxZgztRgA3Y+4ynJOXqCypI58Qyca5XBDndj/N73ZhpZZTQKQo9x4ZeQlgg9Kpkqloecu5CnlJ+qvILc7u50j/7MJQI+WBeWiych71pgGv0R8eINlXrwDQyt4KnN150zMSIAESIAESIIH3m0CtXlIE2sg/fEfnnVHIFjAamYecsIsckwFkDaNdBOnKPNyFklozDuTs1+oQJbn+pNDrKGcLLQGzwNPIWjItentW2rDmtwuyFmtPb+eC7tlpHdSKPliCOJExHPtjsqu48Me11MTWDwpaY/1QySgcT01AKrpyvDLZXue0fNVBKTDTPQM2u5zJVwp93mh3RR6l/n3/g8rw28Da+qMeNsyDhsh51Me+Ffj1nxg8p7TEsuKWBEiABEiABEjgdAnYDLNa4GhcsfKPRlnQvdDdRC1yXNhkNMakTDM+0Ysr0aiXLIHMW3R9zZ8xe2ZBZ/HcXNDdU3KM2eJAnWU1so9Lf9Q64Oe/GAH6Afr/yMqc5Gw/fPLPQpvzuP/XmZ471/gYO1KBQ2QlmWzk5X/0eBLYyyJLmUeW2T5AXqteGNAGpY7PJ73YcCUe1n/7hmHOvxTRE70YUjLWUv1EWGQBeFHvLk9n8tAhunoAlJYUrj93SYAESIAESIAEToeAWZCY9AamhnaxnJ/U3JaFjqb+dbaAsT+rh20XOZoSgHYRZM+pOZgOZJGjKEp81GxmPJrZyM6rOoQe9DpKt2yh8cssxrQPC1HfSFoguvC+LlWYlT/MI51bSNn99lMk914oTbdu6pbt89C9/gtu72nNtyrpd2VTLTjMmwV2rnwECeBnpfVM2cFiw2PuS3b78a+PcFU03U+MEadkYHz9d5zbE033C3MyXwIwP6xk9QHceerMe1H7fO9l9t6MxxH+tb/AfSnzuPcIt/e0N8VygHYxq1QtobRkmSvGNiRAAiRAAiRAAusnIAsSI3R7IbIFiQt/kdJZwDhbJYncr0VmiyA7yNY4Fn4t0gti7CJAx7Hh/iKlnI/qPYTOYk2p6539UqYqMSgLKzvOYk1xo/Crlg7Ac9PpdOrs8ysJkAAJkAAJkAAJkAAJkMAJE5jLdJ+w/Q/EnP3hnvx0i9nm/FnukQAJkAAJkAAJkAAJfCgEmOn+UK4050kCJEACJEACJEACJHBqBOYWUp6aJxyYBEiABEiABEiABEiABN5TAgy639MLy2mRAAmQAAmQAAmQAAmcHQIMus/OtaAnJEACJEACJEACJEAC7ykBBt3v6YXltEiABEiABEiABEiABM4OAQbdZ+da0BMSIAESIAESIAESIIH3lMDJBd2TIaLh5B3BNMEwGuJd8fYdgUo3SYAESIAESIAESIAEKgicXNBdMcB7e3j/x/xPr6uJHqD/nfPz89Im+yn645FQP3ffT1fuXNov+3l5/bPx5+78gP7LlU2zAwmQAAmQAAmQAAmQwIoE+OM4KwLLmm99iSl+xLm9H9Ha+hI+gPH3+7iBT/Hvv2+hLg2lzVbW461+qX/zNdyfGpUg/LMnwP1vryG+ZFyRIPzeD8C3X6Ntj71VLzkYCZAACZAACZAACXwYBEqCbpFexBia+XutNlreRgmNQ6SDPgYqCeuh1arl2kyGEeKZEbRbHjYOUwz6EzTDJnRrLfOotVsoDlHaX43gjgs0m00M4SFsmvFF5mIH9lp63Jxnzo7TNpvnCj5KUP14/xGu9lNM//of3Hyygce3TMAtw0ime/8TTNueGjTpP8LV52b8TSc4B+CeW+mXLGWMvVfaqGNTguyb2EL8zUUAKSIVcBeC60tbiG8dYOxku4/th5kWNyRAAiRAAiRAAiRAAvME5uQlh+kQw2aAMAwRtlvAYFSqfT5MEwxqpl3YxGRgI2wAhymGwyYCsRG20cIAIxFQb3hoNodIrZh6kmLYbM4F3JX9lWl33AAY5scdxDDjhmh7KfqVOvMh4tTT8wwD1AZ9qKbL+mhY+u3PsfP8Kc7dewH/us54z2MG8HIfveebeHzrGqa3vsB9vEC0r1tKgHwVn2NqzrWf/YSeOVdqyx6UTPUejM1reHzhBW5+f2DPzrb7v+D25kW0SrPZF1E3x/N+fA7/yT7lJzOK/EYCJEACJEACJEACxyYwF3Sj7s+yxhs11DDB5LBo/xDjNEXTs9ntGhotnc3VLevws2z2Bmo1YGKMbNQ8DE3UfTiZODbcMar6Lx73cJwibXomiw5s1D14w7T0oQFoIrDZcWj/V/PR+ushvHIewCZaC6Ukf8bdWzYovwjvApD8LAHyAQbPXmNny/K7iNZfzuP2/tE67vHwAA8vf6KkLeKNv7WJh8/+hbF1rWpb1HYrzXjRDw+ty6/RH5YE8VV2eZwESIAESIAESIAESKCUwJy8ZGPjEMOon8lLAA+tua6HmKQeaiJkNp8NFVnbnQ0cDiP0nSS0Z4yoQDid4BAbGKc12OPWjtpuVPVfPO7hJIWXd0o9NORsL9hJ1YPBhg7Wj/LR2nm5r2QlO5dfaZmJkZLY09n20kWk/Uf4zMpLAGx/LGd/Q/rqPDwnC13/0wbwa9az8kv662tAsux3ns7abH40+171TclK9BOCZLc/U2OJH8DtvUe4vTfrqH2c7fMbCZAACZAACZAACZDA6gTmgu5U6zMQqiS21lzPm91AzUt1BtzIvSVrbT+H6QAxRHpilNvDaBbEb9ThIcF4AqS1soBe1ClV/RePK1l0GzgrXw4nFVluOasz+LXMfydgX8JHPdcD9B+IrOQaulspcOcpevseuiUZ75l0Y6bv7ikjF+BtPkUqumor8/h57tWCHq7wv97H57H9sdVt50/mst1bn2Bn7zkGLzG3YFIF7pkf53H/bwXdd94s90iABEiABEiABEiABI5BYE5eojLYNhAVfXepUS0ZsTIRCWBHekWlaq0zzpmRnOwa2EDdAwbxALVMnpIfpLr/4nGLcpKi3CQ/SoqBEprL0QnSoQevbnxewkfppaqVXPjcBNkeutc3cXvvRyT5gdSeBLfbH1/QZ5S+2zYqykmKMg/bbn5bb14EnvwzG08C+/IShSKBAW7cy/umHgSyzLv4Adz4Pytr0eUPgzKN+LwrPEICJEACJEACJEACJLCAwLnpdOpWllNZ5r4NoJsBAsQ6a53pn601t4qIrl4ymJgqIqoCyAA6fGsiCIBYLXA0VUvmKoRYm2a7sL87rrEtCyKNf5Ilz/yvrF4iGfwUaA6zB4Kseol15QgflSxDVSuxOm3dUVf/OI/7UobvpVO9RJXne4GHqtkmHl8HrqpFkNJfAtyfcMMUIXGrl+hxXluvsu2Oyq6bCilHVi8x3dxKJ3JIqp189Ts+q6iw4vqRDcwvJEACJEACJEACJEACKxOYC7pXtnCcDlKqzwmUj2PC9pEgO4FfUdbQtjrG9gR9PMbo7EICJEACJEACJEACJPAeEZjTdK93brYGuIdW21Y+WWVEN8tt+kl5w6aVhaxiq6rtm/pYZZfHSYAESIAESIAESIAEPlQCp5Pp/lBpc94kQAIkQAIkQAIkQAIfJIG5hZQfJAVOmgRIgARIgARIgARIgATWSIBB9xrh0jQJkAAJkAAJkAAJkAAJCAEG3bwPSIAESIAESIAESIAESGDNBBh0rxkwzZMACZAACZAACZAACZAAg27eAyRAAiRAAiRAAiRAAiSwZgIMutcMmOZJgARIgARIgARIgARIgEE37wESIAESIAESIAESIAESWDMBBt1rBkzzJEACJEACJEACJEACJMCgm/cACZAACZAACZAACZAACayZAIPuNQOmeRIgARIgARIgARIgARJg0M17gARIgARIgARIgARIgATWTOAPRftBEBQP5fb9boSuX0MaB+hEuVN6J9xFHHglJ/ShSdJDOGgh6vqoyaFJgl7YQ1LsUWJH9e3NtQT87sxe0U5xX403QCvqwlcOFBugcm7hbowFUysYmiDp3QVulowjPqhThoHbs8ijMDfFfbRovjJuiHG7wtc0RmAuXHE+wvcubqrrW8lafC345Lqvv6eIgw6ikmu4UlvHV+lX9Ffb0vPNbosK3/L3a4jdOED1Xar9x0rXe35mH8aRZa61tOmjvuBvTrE64t4v8lTXFIv/e1Psw30SIAESIAESODUC0wWfg8c70+0Hz0pbPHuwXXmutIM5qGzuPJ4e2EYHj6c72zvTx9kBOXEwfbyzPd3efjB1R5/ra22ssi0dL2+gdG7PHky3t7enFTjyBuye6lOcm0xP5izzc/+ZduqcnneRv57/g+mDne3pTh6YHdHYlv7Ppg9y9mdj5fsKaz22sm/6uG2Ex/Lz1tdu2faKtYxZ7FBkZ5jlm5n7xDmo7Ln313Q6LR7T88zfWzOA5ptzHebO8YCFpP5OHfzlZIrXsqzV3PWdv7bz3Va71+b78wgJkAAJkAAJvD0CZ1ReUoPOqEfoxOmpPZDkBvZa6PpANFzBHy9AvNtAb1DSRzKycYw4+1fMiE8wGgDdls7H6sxzA7vdAEF3F41eiDI06aAnnUwWVzK6ZgzJMpox5U1F1Ue4i0+zNimGUYhmdVo4byodoIcujNv5c8W9NEZn1MWugM19Jkj6ERC2Z28jaj5udn1E/QQT21bGSvyMkRz2gl2ESQ8Z8kkCbWr2VqHmSzY/Qj/JLFmLs23NRzs8os2s9Yf57ahrLZnrIMjerCyCNBkNkPjufVOD3w7lDw4lfz3GlG6TuycWDcJzJEACJEACJHCKBObkJafoS2Fo83+6nSHSwFsgBSh0M7vy6rlf11KY8hYndFS9Eh+jXSVXkMDbEwlEgEwCYYZOikoekWO07AQG6DXaiGtW7uJKIjwEcaRsBg3n9boKMEO0pdMsNJ1NNBmro9Uh96wpYGQD9lBgtUQ+upUyAR0sh+1YS4ds39JtirgzQjcKUJOni9xngnEiMXc+0q/VG4Azh8l4BPgtNHITqqHuA4PxBPBqwGSMBCHypmrQpiToznXOeeG1usDdESb+LGDPNXB3Ku8DI/dpdTHqzWRU5VIZ16D+npP5+F10Gz0MsvvaSol2gU4HkZXWFGUaOZnPAn9qrtTLvd/m/ZL7Sx6MFl7rmo9uHBsJWfEa523W/C7i4rNXvkn5njwM4y5GE3/2gFbekkdJgARIgARI4FQJnOGgW+KhOnwMYOOnUyVls6o384FglU+uPlq3cYJV0Sr3644O3WiIVTp5rJqPJaBEU+vLRyFCP0InC3ztqCG6rT6CnmjkGxjdlaAuRNuedre1OhrQtmeHdQCmHgayJ4AE8izgd7tooBh4Sfu7s+7Fb5MRBkmIdrd4Yn4/jTsYdSMEEhcXT0/GGMFHqxgPF+6HiUTmjXYhbM4H1DYwnzelInNM4BX6O84Is6R/IgFd1Burtw6CRgXSnRjNqgc140L2diPuqodOva8k9Y6TQNTpoxvF6gFNBcN35W1HhFi90dD3VtzMa/zn/Ql00C5vOSSg7oXoxM3q9RkrXOucsyvspMNITbZ47fIm5Hon6I8m8Be8wcn34R4JkAAJkAAJvH0CbxZ0Rx0U48BlM3jLTzVRQXcWGclCzEKGWEtRFv9f8/LjmZZzc3OC5mWMNVpohCEClX1s6B6FDKSdh2IWRZjIFEwEWhcJRC/EsB0jVvMNVDCMuYAdKkMoAVkPPvLJwpJA3b1gyrcYcel8UsQmSBfbeoFlacPZQckq+3XcnB0p/2ZkJZFE3Gf6I1nzwv13TH/9TPID1BqtJR4mRV6UwO/ezN7yaFlMgmLOWNq48WbjZuzse2iGQL/w5DrvD9C6aTP6NTRaPuae0dy5L3ut3T6rfJd7JJK/OetTdeda3Uci88v+I1HdlmdIgARIgARI4LQIvFnQnXttnZ9CvlrEigFrzpSPuhub2VfouTZmp1DtAgh1oIpjjJ/NzWSDG46+2GQCXblIYoNZ069W8xHEu0AgmVITdNvX7cXAWYJxU81kNi0d+PSGMaRMjBV32PM2YJf9cDdCfdxCdBO4m7jZbDdTbbLUZdVUMnxvQ5LjyErsZN7Zrbk3nII6xfvATq2Ru4ntUdkWbdhrpiU2jbZ78+ssvttbvudt11CrFaRBuuBMrlu+j7KS/zvLtT7hncLf6dxDszkvD6Puw8QJe0FzJEACJEACJPBWCbxZ0L3AVS+wGdoFjY46pfS4DeTijkV9RD+t08JKlnEymm6rLe8jadnFjrLQ02SIK7W84qhor0VQkCJOgLpNAXtNhEknl7GXwMMNr2bTbCKIY/PwIKaK0hTT0vOVdnbWr+xbWdZWAjRdzq1uu8gYQ3lkWcMnHSKSApHuU4MaJkEQmYBTSWFKfC3cD5LhxEB06q5EZAIt9dY0izpwO6NyaYo9u+x22ftgkT3HxqJmy54zb1Nm8hK9JqC/bP+30c75O50bTvzvRBXlIeda8wAJkAAJkAAJvDMEzmj1EuFnK1g0s9frp0ZVgmQkGIzm1MeLXZLgtZdgojTKDSeT6KHV9VWFFlu9ZFYtREyKLCRA2EsQKp23BMY95IptSHBSVr6k1KMUcTxRkoGReg3vNJIg2F2MOB6g1wF2g6ZppGUO85lRx4b9Kpprs9DRHprbqoDLrdoSK2075A1BpnHWiyGLlWK0PruePZzogHqA/GUxGWKbWVY68Aj5ojMmMLdt5py0B8RW4U2LPbX2rWaQv17a74VDqweTEO0sRbxEn4UGK04uc60rulYeNg8MjRXro8sDlH/ktawclSdIgARIgARI4K0QOKNBt33lHmJ3+V+jWSMwHSQnvcGC8mXzw8tCML/VgNT+E61zzQThSRwojfRN3EUvjnVZtVxQLRlfHZgGqvKJRME2y27GkRJ69T6CIF7o0zjpIZBMdstTWmLYOUiAE/wv/lcqiBgtr5Tba0TIV2JRC0hlsd7icZRXtQZa/khr8OdxrHDElovrzx40JgnuykNI29H4qjKOSa4koyzQlCoeWclCVfoPuVKDk+QuerLgMwtMK1wrPpBUNFvPYS0vcu857fcyo82uwfJ9lrHrtDmxa21smoBbMvSr/clLSUsfrXwJG8dRfiUBEiABEiCBs0FgbfKS1aYnpcoKqyMl89ldrlJI2Vgib6kuolEyXuUvHmrrNb+NEJ35ig5Kp13mgQkGogkGYYJwt41BZ6BK5NXNSjgpk9aVQNzvYrc1QGeQzkoGGpMSNA1aEcpQqDJrkhWMJRNexipCb9BFFAF3wxj1OEA7DPQcmrLo8X9MBRXnISeXbY7Q6YgePoYPCdJDVUs5KtfBqIVsjRYQyjxK/SnjVHFMMuIyN+e+mF+ka+q590JkP6RaovmXe0F+gXJmyuqmK8Y2h/VDU5Xsp9C38j4otFthV67vrlwvs15AJEi7YYKFUhEv0H3sZOXvaLeBoHMXSWWpxxWcyprKQ8EJXWt5q6Uq7wCQa5mNIV+OWI9hHoyq78mcMe6QAAmQAAmQwKkROCe/w3Nqo7/PA0sw3a+rYLpvflrdTleXfputvsstJFMZv5K632LP/Hx7rr01Klu3r7QfOiXf5JxZrDmJA1mbqeQt7XFoqkTMMunWv7Jx1LmBlCh0Ms6uD2ZhYOXP0OfanuEdl+UZcvOt1Z9fas76Ye30rvVpj78UJDYiARIgARIgAUWAQTdvhJMnoALWHlbV5p68I8e1KJl6kfTka1sf19rx+pm3D4UfP+qFA7RONGN9PO+yXqd4rVWFJDg/DpU5xS8kQAIkQAIkcPYIMOg+e9eEHpGAIWBlPzMg8xKb2Tl+IwESIAESIAESOLsEGHSf3WtDz0iABEiABEiABEiABN4TAme0esl7QpfTIAESIAESIAESIAESIAEADLp5G5AACZAACZAACZAACZDAmgkw6F4zYJonARIgARIgARIgARIgAQbdvAdIgARIgARIgARIgARIYM0EGHSvGTDNkwAJkAAJkAAJkAAJkACDbt4DJEACJEACJEACJEACJLBmAv8PJ3JmrrMwIIMAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF+SVM模型+网格搜索交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM模型训练太耗时，尤其是使用网格搜索训练\n",
    "* 参数调优时，使用param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}+5折交叉验证，连续训练时长将近48小时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最好的参数：\n",
      "{'C': 10, 'gamma': 1}\n",
      "最好的得分：\n",
      "0.9517343039999999\n",
      "网格搜索参数及得分：\n",
      "[mean: 0.95173, std: 0.00264, params: {'C': 10, 'gamma': 1}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "'''\n",
    "线性的SVM只需要，只需要调优正则化参数C\n",
    "基于RBF核的SVM，需要调优gamma参数和C\n",
    "'''\n",
    "param_grid = {'C': [10],'gamma': [1]}\n",
    "\n",
    "model_SVM = GridSearchCV(SVC(), param_grid, scoring='roc_auc', cv=5)\n",
    "model_SVM.fit(tfidf_train_x, label)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最好的参数：\")\n",
    "print( model_SVM.best_params_)\n",
    "\n",
    "print(\"最好的得分：\")\n",
    "print(model_SVM.best_score_)\n",
    "\n",
    "print(\"网格搜索参数及得分：\")\n",
    "print(model_SVM.grid_scores_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(model_SVM, \"model_SVM\")\n",
    "model_SVM = joblib.load(\"model_SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='linear',C=10,gamma = 1)\n",
    "svm.fit(tfidf_train_x, label)\n",
    "\n",
    "test_predicted = np.array(svm.predict(tfidf_test_x))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_svm_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. TF-IDF+MLP（多层感知机模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 10)                40010     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 40,032\n",
      "Trainable params: 40,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 17500 samples, validate on 7500 samples\n",
      "Epoch 1/500\n",
      "17500/17500 [==============================] - 1s 61us/step - loss: 0.6908 - acc: 0.5581 - val_loss: 0.6856 - val_acc: 0.6764\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67640, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 2/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.6817 - acc: 0.6987 - val_loss: 0.6745 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.67640 to 0.77120, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 3/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.6692 - acc: 0.7624 - val_loss: 0.6611 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.77120 to 0.80707, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 4/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.6550 - acc: 0.7938 - val_loss: 0.6477 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80707 to 0.82773, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 5/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.6418 - acc: 0.8066 - val_loss: 0.6346 - val_acc: 0.8359\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82773 to 0.83587, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 6/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.6281 - acc: 0.8171 - val_loss: 0.6217 - val_acc: 0.8369\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.83587 to 0.83693, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 7/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.6149 - acc: 0.8209 - val_loss: 0.6089 - val_acc: 0.8401\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.83693 to 0.84013, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 8/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.6011 - acc: 0.8323 - val_loss: 0.5965 - val_acc: 0.8415\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.84013 to 0.84147, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 9/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.5881 - acc: 0.8338 - val_loss: 0.5843 - val_acc: 0.8412\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.84147\n",
      "Epoch 10/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.5755 - acc: 0.8350 - val_loss: 0.5725 - val_acc: 0.8419\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.84147 to 0.84187, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 11/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.5631 - acc: 0.8406 - val_loss: 0.5609 - val_acc: 0.8424\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.84187 to 0.84240, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 12/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.5516 - acc: 0.8417 - val_loss: 0.5496 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84240 to 0.84347, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 13/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.5403 - acc: 0.8417 - val_loss: 0.5386 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.84347 to 0.84520, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 14/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.5287 - acc: 0.8470 - val_loss: 0.5279 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.84520 to 0.84627, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 15/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.5177 - acc: 0.8456 - val_loss: 0.5176 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.84627 to 0.84747, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 16/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.5073 - acc: 0.8466 - val_loss: 0.5076 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.84747 to 0.84773, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 17/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4961 - acc: 0.8530 - val_loss: 0.4979 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.84773 to 0.84853, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 18/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4866 - acc: 0.8547 - val_loss: 0.4886 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.84853 to 0.84960, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 19/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4771 - acc: 0.8526 - val_loss: 0.4797 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.84960 to 0.85080, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 20/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4684 - acc: 0.8599 - val_loss: 0.4711 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.85080 to 0.85160, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 21/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.4575 - acc: 0.8591 - val_loss: 0.4629 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.85160 to 0.85227, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 22/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4497 - acc: 0.8609 - val_loss: 0.4549 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00022: val_acc improved from 0.85227 to 0.85347, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 23/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4417 - acc: 0.8625 - val_loss: 0.4473 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.85347 to 0.85467, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 24/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.4333 - acc: 0.8638 - val_loss: 0.4401 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.85467 to 0.85560, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 25/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.4257 - acc: 0.8643 - val_loss: 0.4332 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.85560 to 0.85707, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 26/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.4180 - acc: 0.8694 - val_loss: 0.4266 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.85707 to 0.85840, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 27/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.4109 - acc: 0.8686 - val_loss: 0.4203 - val_acc: 0.8585\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.85840 to 0.85853, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 28/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.4055 - acc: 0.8694 - val_loss: 0.4143 - val_acc: 0.8588\n",
      "\n",
      "Epoch 00028: val_acc improved from 0.85853 to 0.85880, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 29/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3995 - acc: 0.8695 - val_loss: 0.4086 - val_acc: 0.8592\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.85880 to 0.85920, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 30/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3913 - acc: 0.8742 - val_loss: 0.4031 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00030: val_acc improved from 0.85920 to 0.86000, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 31/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3854 - acc: 0.8743 - val_loss: 0.3979 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00031: val_acc improved from 0.86000 to 0.86067, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 32/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3807 - acc: 0.8782 - val_loss: 0.3929 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.86067 to 0.86147, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 33/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3745 - acc: 0.8783 - val_loss: 0.3881 - val_acc: 0.8625\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.86147 to 0.86253, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 34/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3700 - acc: 0.8789 - val_loss: 0.3836 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.86253 to 0.86267, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 35/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3636 - acc: 0.8813 - val_loss: 0.3793 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00035: val_acc improved from 0.86267 to 0.86307, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 36/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3589 - acc: 0.8809 - val_loss: 0.3752 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.86307 to 0.86333, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 37/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3551 - acc: 0.8802 - val_loss: 0.3713 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00037: val_acc improved from 0.86333 to 0.86413, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 38/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3497 - acc: 0.8815 - val_loss: 0.3675 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.86413 to 0.86507, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 39/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3448 - acc: 0.8849 - val_loss: 0.3640 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00039: val_acc improved from 0.86507 to 0.86560, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 40/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3405 - acc: 0.8866 - val_loss: 0.3606 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00040: val_acc improved from 0.86560 to 0.86613, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 41/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3371 - acc: 0.8853 - val_loss: 0.3574 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.86613 to 0.86667, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 42/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3325 - acc: 0.8859 - val_loss: 0.3543 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00042: val_acc improved from 0.86667 to 0.86680, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 43/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3284 - acc: 0.8891 - val_loss: 0.3513 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00043: val_acc improved from 0.86680 to 0.86747, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 44/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3252 - acc: 0.8885 - val_loss: 0.3485 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.86747\n",
      "Epoch 45/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3209 - acc: 0.8937 - val_loss: 0.3459 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.86747\n",
      "Epoch 46/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3178 - acc: 0.8927 - val_loss: 0.3433 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00046: val_acc improved from 0.86747 to 0.86813, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 47/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3149 - acc: 0.8923 - val_loss: 0.3409 - val_acc: 0.8681\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.86813\n",
      "Epoch 48/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3095 - acc: 0.8950 - val_loss: 0.3386 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00048: val_acc improved from 0.86813 to 0.86907, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 49/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3087 - acc: 0.8928 - val_loss: 0.3364 - val_acc: 0.8695\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.86907 to 0.86947, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 50/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3048 - acc: 0.8954 - val_loss: 0.3343 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.86947 to 0.86987, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 51/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.3035 - acc: 0.8936 - val_loss: 0.3323 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.86987 to 0.87093, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 52/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2987 - acc: 0.8949 - val_loss: 0.3304 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.87093\n",
      "Epoch 53/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2956 - acc: 0.8969 - val_loss: 0.3286 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00053: val_acc improved from 0.87093 to 0.87107, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 54/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2938 - acc: 0.8994 - val_loss: 0.3268 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.87107\n",
      "Epoch 55/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2897 - acc: 0.8982 - val_loss: 0.3251 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.87107\n",
      "Epoch 56/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2885 - acc: 0.8987 - val_loss: 0.3235 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.87107\n",
      "Epoch 57/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2864 - acc: 0.8997 - val_loss: 0.3220 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.87107\n",
      "Epoch 58/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2836 - acc: 0.9011 - val_loss: 0.3205 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.87107\n",
      "Epoch 59/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2805 - acc: 0.9025 - val_loss: 0.3191 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00059: val_acc improved from 0.87107 to 0.87120, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 60/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2791 - acc: 0.9005 - val_loss: 0.3178 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00060: val_acc improved from 0.87120 to 0.87133, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 61/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2769 - acc: 0.9037 - val_loss: 0.3165 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00061: val_acc improved from 0.87133 to 0.87160, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 62/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2749 - acc: 0.9048 - val_loss: 0.3153 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00062: val_acc improved from 0.87160 to 0.87187, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 63/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2714 - acc: 0.9063 - val_loss: 0.3142 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00063: val_acc improved from 0.87187 to 0.87200, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 64/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2698 - acc: 0.9057 - val_loss: 0.3130 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.87200 to 0.87240, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 65/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2683 - acc: 0.9051 - val_loss: 0.3120 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.87240\n",
      "Epoch 66/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2668 - acc: 0.9073 - val_loss: 0.3109 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00066: val_acc improved from 0.87240 to 0.87320, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 67/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2649 - acc: 0.9071 - val_loss: 0.3100 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00067: val_acc improved from 0.87320 to 0.87400, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 68/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.2633 - acc: 0.9078 - val_loss: 0.3091 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00068: val_acc improved from 0.87400 to 0.87427, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 69/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2610 - acc: 0.9079 - val_loss: 0.3083 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.87427\n",
      "Epoch 70/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2589 - acc: 0.9078 - val_loss: 0.3075 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.87427\n",
      "Epoch 71/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.2557 - acc: 0.9075 - val_loss: 0.3067 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.87427\n",
      "Epoch 72/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2549 - acc: 0.9093 - val_loss: 0.3059 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00072: val_acc did not improve from 0.87427\n",
      "Epoch 73/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2541 - acc: 0.9108 - val_loss: 0.3053 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00073: val_acc improved from 0.87427 to 0.87440, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 74/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2530 - acc: 0.9099 - val_loss: 0.3046 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.87440\n",
      "Epoch 75/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2520 - acc: 0.9086 - val_loss: 0.3040 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00075: val_acc improved from 0.87440 to 0.87440, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 76/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2503 - acc: 0.9101 - val_loss: 0.3035 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.87440\n",
      "Epoch 77/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.2467 - acc: 0.9131 - val_loss: 0.3030 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00077: val_acc improved from 0.87440 to 0.87453, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 78/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2445 - acc: 0.9133 - val_loss: 0.3024 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.87453\n",
      "Epoch 79/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2436 - acc: 0.9149 - val_loss: 0.3019 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.87453\n",
      "Epoch 80/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2428 - acc: 0.9138 - val_loss: 0.3015 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.87453\n",
      "Epoch 81/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2411 - acc: 0.9147 - val_loss: 0.3011 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.87453\n",
      "Epoch 82/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2404 - acc: 0.9144 - val_loss: 0.3006 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.87453\n",
      "Epoch 83/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2376 - acc: 0.9177 - val_loss: 0.3002 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.87453\n",
      "Epoch 84/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2375 - acc: 0.9154 - val_loss: 0.2999 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.87453\n",
      "Epoch 85/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2351 - acc: 0.9141 - val_loss: 0.2996 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.87453\n",
      "Epoch 86/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2350 - acc: 0.9166 - val_loss: 0.2993 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.87453\n",
      "Epoch 87/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2306 - acc: 0.9203 - val_loss: 0.2990 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.87453\n",
      "Epoch 88/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2303 - acc: 0.9187 - val_loss: 0.2988 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.87453\n",
      "Epoch 89/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2293 - acc: 0.9183 - val_loss: 0.2985 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00089: val_acc did not improve from 0.87453\n",
      "Epoch 90/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2288 - acc: 0.9179 - val_loss: 0.2983 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.87453\n",
      "Epoch 91/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2275 - acc: 0.9187 - val_loss: 0.2982 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.87453\n",
      "Epoch 92/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2287 - acc: 0.9174 - val_loss: 0.2980 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00092: val_acc improved from 0.87453 to 0.87467, saving model to model_MLP/weights.best.hdf5\n",
      "Epoch 93/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2255 - acc: 0.9207 - val_loss: 0.2979 - val_acc: 0.8745\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.87467\n",
      "Epoch 94/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2233 - acc: 0.9187 - val_loss: 0.2978 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.87467\n",
      "Epoch 95/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2221 - acc: 0.9197 - val_loss: 0.2976 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.87467\n",
      "Epoch 96/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2215 - acc: 0.9206 - val_loss: 0.2975 - val_acc: 0.8737\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.87467\n",
      "Epoch 97/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2196 - acc: 0.9231 - val_loss: 0.2974 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.87467\n",
      "Epoch 98/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2208 - acc: 0.9213 - val_loss: 0.2974 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.87467\n",
      "Epoch 99/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2205 - acc: 0.9223 - val_loss: 0.2973 - val_acc: 0.8741\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.87467\n",
      "Epoch 100/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2171 - acc: 0.9238 - val_loss: 0.2974 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.87467\n",
      "Epoch 101/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2158 - acc: 0.9245 - val_loss: 0.2974 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.87467\n",
      "Epoch 102/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2156 - acc: 0.9246 - val_loss: 0.2974 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.87467\n",
      "Epoch 103/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2149 - acc: 0.9249 - val_loss: 0.2974 - val_acc: 0.8731\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.87467\n",
      "Epoch 104/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2126 - acc: 0.9246 - val_loss: 0.2974 - val_acc: 0.8737\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.87467\n",
      "Epoch 105/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2112 - acc: 0.9265 - val_loss: 0.2975 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.87467\n",
      "Epoch 106/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2133 - acc: 0.9234 - val_loss: 0.2975 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00106: val_acc did not improve from 0.87467\n",
      "Epoch 107/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2100 - acc: 0.9274 - val_loss: 0.2976 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.87467\n",
      "Epoch 108/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2089 - acc: 0.9280 - val_loss: 0.2977 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.87467\n",
      "Epoch 109/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2072 - acc: 0.9271 - val_loss: 0.2978 - val_acc: 0.8739\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.87467\n",
      "Epoch 110/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2094 - acc: 0.9256 - val_loss: 0.2979 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.87467\n",
      "Epoch 111/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2078 - acc: 0.9255 - val_loss: 0.2980 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.87467\n",
      "Epoch 112/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2054 - acc: 0.9269 - val_loss: 0.2981 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.87467\n",
      "Epoch 113/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2042 - acc: 0.9279 - val_loss: 0.2983 - val_acc: 0.8732\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.87467\n",
      "Epoch 114/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2053 - acc: 0.9266 - val_loss: 0.2984 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.87467\n",
      "Epoch 115/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2023 - acc: 0.9285 - val_loss: 0.2986 - val_acc: 0.8727\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.87467\n",
      "Epoch 116/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2015 - acc: 0.9292 - val_loss: 0.2988 - val_acc: 0.8731\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.87467\n",
      "Epoch 117/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2012 - acc: 0.9282 - val_loss: 0.2989 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.87467\n",
      "Epoch 118/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2008 - acc: 0.9280 - val_loss: 0.2991 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.87467\n",
      "Epoch 119/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.2006 - acc: 0.9296 - val_loss: 0.2993 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.87467\n",
      "Epoch 120/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1995 - acc: 0.9309 - val_loss: 0.2995 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.87467\n",
      "Epoch 121/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1969 - acc: 0.9300 - val_loss: 0.2997 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.87467\n",
      "Epoch 122/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1975 - acc: 0.9303 - val_loss: 0.2999 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.87467\n",
      "Epoch 123/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1981 - acc: 0.9289 - val_loss: 0.3001 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.87467\n",
      "Epoch 124/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1952 - acc: 0.9300 - val_loss: 0.3004 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.87467\n",
      "Epoch 125/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1956 - acc: 0.9297 - val_loss: 0.3006 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.87467\n",
      "Epoch 126/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1955 - acc: 0.9307 - val_loss: 0.3009 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.87467\n",
      "Epoch 127/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1921 - acc: 0.9331 - val_loss: 0.3011 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.87467\n",
      "Epoch 128/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1927 - acc: 0.9311 - val_loss: 0.3014 - val_acc: 0.8728\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.87467\n",
      "Epoch 129/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1918 - acc: 0.9331 - val_loss: 0.3017 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.87467\n",
      "Epoch 130/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1918 - acc: 0.9325 - val_loss: 0.3020 - val_acc: 0.8721\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.87467\n",
      "Epoch 131/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1917 - acc: 0.9310 - val_loss: 0.3023 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.87467\n",
      "Epoch 132/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1896 - acc: 0.9332 - val_loss: 0.3026 - val_acc: 0.8720\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.87467\n",
      "Epoch 133/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1879 - acc: 0.9346 - val_loss: 0.3029 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.87467\n",
      "Epoch 134/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1882 - acc: 0.9350 - val_loss: 0.3032 - val_acc: 0.8719\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.87467\n",
      "Epoch 135/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1865 - acc: 0.9362 - val_loss: 0.3035 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.87467\n",
      "Epoch 136/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1866 - acc: 0.9323 - val_loss: 0.3038 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.87467\n",
      "Epoch 137/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1859 - acc: 0.9342 - val_loss: 0.3042 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.87467\n",
      "Epoch 138/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1851 - acc: 0.9339 - val_loss: 0.3045 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.87467\n",
      "Epoch 139/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1863 - acc: 0.9349 - val_loss: 0.3048 - val_acc: 0.8711\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.87467\n",
      "Epoch 140/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1843 - acc: 0.9351 - val_loss: 0.3052 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.87467\n",
      "Epoch 141/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1832 - acc: 0.9342 - val_loss: 0.3056 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.87467\n",
      "Epoch 142/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1820 - acc: 0.9376 - val_loss: 0.3059 - val_acc: 0.8712\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.87467\n",
      "Epoch 143/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1818 - acc: 0.9364 - val_loss: 0.3063 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.87467\n",
      "Epoch 144/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1801 - acc: 0.9383 - val_loss: 0.3067 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.87467\n",
      "Epoch 145/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1818 - acc: 0.9366 - val_loss: 0.3072 - val_acc: 0.8708\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.87467\n",
      "Epoch 146/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1812 - acc: 0.9358 - val_loss: 0.3076 - val_acc: 0.8699\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.87467\n",
      "Epoch 147/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1788 - acc: 0.9364 - val_loss: 0.3080 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.87467\n",
      "Epoch 148/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1782 - acc: 0.9392 - val_loss: 0.3084 - val_acc: 0.8700\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.87467\n",
      "Epoch 149/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1774 - acc: 0.9393 - val_loss: 0.3088 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.87467\n",
      "Epoch 150/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1776 - acc: 0.9367 - val_loss: 0.3093 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.87467\n",
      "Epoch 151/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1774 - acc: 0.9371 - val_loss: 0.3097 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.87467\n",
      "Epoch 152/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1753 - acc: 0.9399 - val_loss: 0.3101 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.87467\n",
      "Epoch 153/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1766 - acc: 0.9366 - val_loss: 0.3106 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.87467\n",
      "Epoch 154/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1768 - acc: 0.9385 - val_loss: 0.3109 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.87467\n",
      "Epoch 155/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1743 - acc: 0.9376 - val_loss: 0.3114 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.87467\n",
      "Epoch 156/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1742 - acc: 0.9410 - val_loss: 0.3118 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00156: val_acc did not improve from 0.87467\n",
      "Epoch 157/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1729 - acc: 0.9406 - val_loss: 0.3122 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.87467\n",
      "Epoch 158/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1735 - acc: 0.9409 - val_loss: 0.3127 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.87467\n",
      "Epoch 159/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1712 - acc: 0.9399 - val_loss: 0.3131 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.87467\n",
      "Epoch 160/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1696 - acc: 0.9403 - val_loss: 0.3136 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.87467\n",
      "Epoch 161/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1734 - acc: 0.9401 - val_loss: 0.3141 - val_acc: 0.8687\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.87467\n",
      "Epoch 162/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1691 - acc: 0.9390 - val_loss: 0.3145 - val_acc: 0.8683\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.87467\n",
      "Epoch 163/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1708 - acc: 0.9396 - val_loss: 0.3149 - val_acc: 0.8677\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.87467\n",
      "Epoch 164/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1688 - acc: 0.9405 - val_loss: 0.3154 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.87467\n",
      "Epoch 165/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1683 - acc: 0.9417 - val_loss: 0.3159 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.87467\n",
      "Epoch 166/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1683 - acc: 0.9414 - val_loss: 0.3163 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.87467\n",
      "Epoch 167/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1668 - acc: 0.9424 - val_loss: 0.3168 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.87467\n",
      "Epoch 168/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1682 - acc: 0.9424 - val_loss: 0.3172 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.87467\n",
      "Epoch 169/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1654 - acc: 0.9434 - val_loss: 0.3177 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.87467\n",
      "Epoch 170/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1644 - acc: 0.9444 - val_loss: 0.3182 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.87467\n",
      "Epoch 171/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1653 - acc: 0.9438 - val_loss: 0.3188 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.87467\n",
      "Epoch 172/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1645 - acc: 0.9427 - val_loss: 0.3193 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.87467\n",
      "Epoch 173/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1639 - acc: 0.9429 - val_loss: 0.3199 - val_acc: 0.8677\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.87467\n",
      "Epoch 174/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1649 - acc: 0.9425 - val_loss: 0.3204 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.87467\n",
      "Epoch 175/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1633 - acc: 0.9424 - val_loss: 0.3209 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.87467\n",
      "Epoch 176/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1637 - acc: 0.9437 - val_loss: 0.3214 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.87467\n",
      "Epoch 177/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1644 - acc: 0.9436 - val_loss: 0.3219 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.87467\n",
      "Epoch 178/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1638 - acc: 0.9439 - val_loss: 0.3223 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.87467\n",
      "Epoch 179/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1620 - acc: 0.9429 - val_loss: 0.3228 - val_acc: 0.8667\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.87467\n",
      "Epoch 180/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1620 - acc: 0.9443 - val_loss: 0.3233 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.87467\n",
      "Epoch 181/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1617 - acc: 0.9434 - val_loss: 0.3238 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.87467\n",
      "Epoch 182/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1601 - acc: 0.9433 - val_loss: 0.3243 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.87467\n",
      "Epoch 183/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1596 - acc: 0.9462 - val_loss: 0.3249 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.87467\n",
      "Epoch 184/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1603 - acc: 0.9440 - val_loss: 0.3254 - val_acc: 0.8664\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.87467\n",
      "Epoch 185/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1586 - acc: 0.9446 - val_loss: 0.3259 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.87467\n",
      "Epoch 186/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1573 - acc: 0.9458 - val_loss: 0.3265 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.87467\n",
      "Epoch 187/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1570 - acc: 0.9452 - val_loss: 0.3271 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.87467\n",
      "Epoch 188/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1585 - acc: 0.9450 - val_loss: 0.3276 - val_acc: 0.8651\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.87467\n",
      "Epoch 189/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1572 - acc: 0.9464 - val_loss: 0.3281 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.87467\n",
      "Epoch 190/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1565 - acc: 0.9462 - val_loss: 0.3287 - val_acc: 0.8648\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.87467\n",
      "Epoch 191/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1560 - acc: 0.9453 - val_loss: 0.3292 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.87467\n",
      "Epoch 192/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1536 - acc: 0.9478 - val_loss: 0.3299 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.87467\n",
      "Epoch 193/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1539 - acc: 0.9464 - val_loss: 0.3305 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.87467\n",
      "Epoch 194/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1541 - acc: 0.9461 - val_loss: 0.3311 - val_acc: 0.8644\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.87467\n",
      "Epoch 195/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1538 - acc: 0.9482 - val_loss: 0.3317 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.87467\n",
      "Epoch 196/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1528 - acc: 0.9471 - val_loss: 0.3323 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.87467\n",
      "Epoch 197/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1519 - acc: 0.9484 - val_loss: 0.3328 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.87467\n",
      "Epoch 198/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1530 - acc: 0.9458 - val_loss: 0.3334 - val_acc: 0.8639\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.87467\n",
      "Epoch 199/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1519 - acc: 0.9485 - val_loss: 0.3340 - val_acc: 0.8636\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.87467\n",
      "Epoch 200/500\n",
      "17500/17500 [==============================] - 1s 47us/step - loss: 0.1518 - acc: 0.9472 - val_loss: 0.3346 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.87467\n",
      "Epoch 201/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1508 - acc: 0.9468 - val_loss: 0.3351 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00201: val_acc did not improve from 0.87467\n",
      "Epoch 202/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1512 - acc: 0.9481 - val_loss: 0.3356 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00202: val_acc did not improve from 0.87467\n",
      "Epoch 203/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1505 - acc: 0.9483 - val_loss: 0.3362 - val_acc: 0.8631\n",
      "\n",
      "Epoch 00203: val_acc did not improve from 0.87467\n",
      "Epoch 204/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1519 - acc: 0.9483 - val_loss: 0.3366 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00204: val_acc did not improve from 0.87467\n",
      "Epoch 205/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1509 - acc: 0.9474 - val_loss: 0.3371 - val_acc: 0.8637\n",
      "\n",
      "Epoch 00205: val_acc did not improve from 0.87467\n",
      "Epoch 206/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1500 - acc: 0.9489 - val_loss: 0.3376 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00206: val_acc did not improve from 0.87467\n",
      "Epoch 207/500\n",
      "17500/17500 [==============================] - 1s 63us/step - loss: 0.1490 - acc: 0.9473 - val_loss: 0.3381 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00207: val_acc did not improve from 0.87467\n",
      "Epoch 208/500\n",
      "17500/17500 [==============================] - 1s 66us/step - loss: 0.1484 - acc: 0.9494 - val_loss: 0.3386 - val_acc: 0.8633\n",
      "\n",
      "Epoch 00208: val_acc did not improve from 0.87467\n",
      "Epoch 209/500\n",
      "17500/17500 [==============================] - 1s 54us/step - loss: 0.1486 - acc: 0.9489 - val_loss: 0.3392 - val_acc: 0.8629\n",
      "\n",
      "Epoch 00209: val_acc did not improve from 0.87467\n",
      "Epoch 210/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1486 - acc: 0.9483 - val_loss: 0.3399 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00210: val_acc did not improve from 0.87467\n",
      "Epoch 211/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1471 - acc: 0.9497 - val_loss: 0.3405 - val_acc: 0.8621\n",
      "\n",
      "Epoch 00211: val_acc did not improve from 0.87467\n",
      "Epoch 212/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.1465 - acc: 0.9484 - val_loss: 0.3410 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00212: val_acc did not improve from 0.87467\n",
      "Epoch 213/500\n",
      "17500/17500 [==============================] - 1s 49us/step - loss: 0.1473 - acc: 0.9496 - val_loss: 0.3416 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00213: val_acc did not improve from 0.87467\n",
      "Epoch 214/500\n",
      "17500/17500 [==============================] - 1s 49us/step - loss: 0.1445 - acc: 0.9489 - val_loss: 0.3422 - val_acc: 0.8617\n",
      "\n",
      "Epoch 00214: val_acc did not improve from 0.87467\n",
      "Epoch 215/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1459 - acc: 0.9498 - val_loss: 0.3428 - val_acc: 0.8616\n",
      "\n",
      "Epoch 00215: val_acc did not improve from 0.87467\n",
      "Epoch 216/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1460 - acc: 0.9493 - val_loss: 0.3433 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00216: val_acc did not improve from 0.87467\n",
      "Epoch 217/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1446 - acc: 0.9491 - val_loss: 0.3439 - val_acc: 0.8607\n",
      "\n",
      "Epoch 00217: val_acc did not improve from 0.87467\n",
      "Epoch 218/500\n",
      "17500/17500 [==============================] - 1s 57us/step - loss: 0.1454 - acc: 0.9483 - val_loss: 0.3445 - val_acc: 0.8605\n",
      "\n",
      "Epoch 00218: val_acc did not improve from 0.87467\n",
      "Epoch 219/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1449 - acc: 0.9507 - val_loss: 0.3451 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00219: val_acc did not improve from 0.87467\n",
      "Epoch 220/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1442 - acc: 0.9489 - val_loss: 0.3456 - val_acc: 0.8603\n",
      "\n",
      "Epoch 00220: val_acc did not improve from 0.87467\n",
      "Epoch 221/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1430 - acc: 0.9515 - val_loss: 0.3462 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00221: val_acc did not improve from 0.87467\n",
      "Epoch 222/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1429 - acc: 0.9518 - val_loss: 0.3469 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00222: val_acc did not improve from 0.87467\n",
      "Epoch 223/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1430 - acc: 0.9502 - val_loss: 0.3476 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00223: val_acc did not improve from 0.87467\n",
      "Epoch 224/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1412 - acc: 0.9507 - val_loss: 0.3482 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00224: val_acc did not improve from 0.87467\n",
      "Epoch 225/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1425 - acc: 0.9511 - val_loss: 0.3489 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00225: val_acc did not improve from 0.87467\n",
      "Epoch 226/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1410 - acc: 0.9512 - val_loss: 0.3496 - val_acc: 0.8599\n",
      "\n",
      "Epoch 00226: val_acc did not improve from 0.87467\n",
      "Epoch 227/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1414 - acc: 0.9498 - val_loss: 0.3502 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00227: val_acc did not improve from 0.87467\n",
      "Epoch 228/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1407 - acc: 0.9530 - val_loss: 0.3508 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00228: val_acc did not improve from 0.87467\n",
      "Epoch 229/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1402 - acc: 0.9509 - val_loss: 0.3513 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00229: val_acc did not improve from 0.87467\n",
      "Epoch 230/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1394 - acc: 0.9518 - val_loss: 0.3519 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00230: val_acc did not improve from 0.87467\n",
      "Epoch 231/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1389 - acc: 0.9529 - val_loss: 0.3526 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00231: val_acc did not improve from 0.87467\n",
      "Epoch 232/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1392 - acc: 0.9521 - val_loss: 0.3533 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00232: val_acc did not improve from 0.87467\n",
      "Epoch 233/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1374 - acc: 0.9539 - val_loss: 0.3540 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00233: val_acc did not improve from 0.87467\n",
      "Epoch 234/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1387 - acc: 0.9517 - val_loss: 0.3547 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00234: val_acc did not improve from 0.87467\n",
      "Epoch 235/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1382 - acc: 0.9526 - val_loss: 0.3554 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00235: val_acc did not improve from 0.87467\n",
      "Epoch 236/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1381 - acc: 0.9519 - val_loss: 0.3561 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00236: val_acc did not improve from 0.87467\n",
      "Epoch 237/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.1382 - acc: 0.9530 - val_loss: 0.3567 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00237: val_acc did not improve from 0.87467\n",
      "Epoch 238/500\n",
      "17500/17500 [==============================] - 1s 48us/step - loss: 0.1349 - acc: 0.9538 - val_loss: 0.3573 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00238: val_acc did not improve from 0.87467\n",
      "Epoch 239/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1356 - acc: 0.9530 - val_loss: 0.3580 - val_acc: 0.8584\n",
      "\n",
      "Epoch 00239: val_acc did not improve from 0.87467\n",
      "Epoch 240/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1352 - acc: 0.9530 - val_loss: 0.3587 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00240: val_acc did not improve from 0.87467\n",
      "Epoch 241/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1358 - acc: 0.9533 - val_loss: 0.3594 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00241: val_acc did not improve from 0.87467\n",
      "Epoch 242/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1356 - acc: 0.9531 - val_loss: 0.3599 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00242: val_acc did not improve from 0.87467\n",
      "Epoch 243/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1334 - acc: 0.9538 - val_loss: 0.3606 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00243: val_acc did not improve from 0.87467\n",
      "Epoch 244/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1341 - acc: 0.9533 - val_loss: 0.3613 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00244: val_acc did not improve from 0.87467\n",
      "Epoch 245/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1333 - acc: 0.9535 - val_loss: 0.3620 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00245: val_acc did not improve from 0.87467\n",
      "Epoch 246/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1370 - acc: 0.9516 - val_loss: 0.3626 - val_acc: 0.8579\n",
      "\n",
      "Epoch 00246: val_acc did not improve from 0.87467\n",
      "Epoch 247/500\n",
      "17500/17500 [==============================] - 1s 51us/step - loss: 0.1325 - acc: 0.9545 - val_loss: 0.3632 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00247: val_acc did not improve from 0.87467\n",
      "Epoch 248/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1330 - acc: 0.9538 - val_loss: 0.3638 - val_acc: 0.8569\n",
      "\n",
      "Epoch 00248: val_acc did not improve from 0.87467\n",
      "Epoch 249/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1326 - acc: 0.9532 - val_loss: 0.3644 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00249: val_acc did not improve from 0.87467\n",
      "Epoch 250/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1347 - acc: 0.9521 - val_loss: 0.3651 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00250: val_acc did not improve from 0.87467\n",
      "Epoch 251/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1313 - acc: 0.9545 - val_loss: 0.3657 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00251: val_acc did not improve from 0.87467\n",
      "Epoch 252/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1320 - acc: 0.9553 - val_loss: 0.3664 - val_acc: 0.8577\n",
      "\n",
      "Epoch 00252: val_acc did not improve from 0.87467\n",
      "Epoch 253/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1316 - acc: 0.9552 - val_loss: 0.3670 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00253: val_acc did not improve from 0.87467\n",
      "Epoch 254/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1318 - acc: 0.9542 - val_loss: 0.3677 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00254: val_acc did not improve from 0.87467\n",
      "Epoch 255/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1318 - acc: 0.9549 - val_loss: 0.3684 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00255: val_acc did not improve from 0.87467\n",
      "Epoch 256/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1309 - acc: 0.9555 - val_loss: 0.3690 - val_acc: 0.8571\n",
      "\n",
      "Epoch 00256: val_acc did not improve from 0.87467\n",
      "Epoch 257/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1298 - acc: 0.9543 - val_loss: 0.3696 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00257: val_acc did not improve from 0.87467\n",
      "Epoch 258/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1306 - acc: 0.9535 - val_loss: 0.3702 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00258: val_acc did not improve from 0.87467\n",
      "Epoch 259/500\n",
      "17500/17500 [==============================] - 1s 50us/step - loss: 0.1286 - acc: 0.9566 - val_loss: 0.3709 - val_acc: 0.8573\n",
      "\n",
      "Epoch 00259: val_acc did not improve from 0.87467\n",
      "Epoch 260/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1297 - acc: 0.9545 - val_loss: 0.3714 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00260: val_acc did not improve from 0.87467\n",
      "Epoch 261/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1288 - acc: 0.9545 - val_loss: 0.3721 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00261: val_acc did not improve from 0.87467\n",
      "Epoch 262/500\n",
      "17500/17500 [==============================] - 1s 48us/step - loss: 0.1279 - acc: 0.9558 - val_loss: 0.3727 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00262: val_acc did not improve from 0.87467\n",
      "Epoch 263/500\n",
      "17500/17500 [==============================] - 1s 59us/step - loss: 0.1284 - acc: 0.9560 - val_loss: 0.3734 - val_acc: 0.8565\n",
      "\n",
      "Epoch 00263: val_acc did not improve from 0.87467\n",
      "Epoch 264/500\n",
      "17500/17500 [==============================] - 1s 57us/step - loss: 0.1278 - acc: 0.9562 - val_loss: 0.3741 - val_acc: 0.8569\n",
      "\n",
      "Epoch 00264: val_acc did not improve from 0.87467\n",
      "Epoch 265/500\n",
      "17500/17500 [==============================] - 1s 56us/step - loss: 0.1273 - acc: 0.9558 - val_loss: 0.3747 - val_acc: 0.8567\n",
      "\n",
      "Epoch 00265: val_acc did not improve from 0.87467\n",
      "Epoch 266/500\n",
      "17500/17500 [==============================] - 1s 51us/step - loss: 0.1268 - acc: 0.9564 - val_loss: 0.3754 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00266: val_acc did not improve from 0.87467\n",
      "Epoch 267/500\n",
      "17500/17500 [==============================] - 1s 49us/step - loss: 0.1257 - acc: 0.9574 - val_loss: 0.3761 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00267: val_acc did not improve from 0.87467\n",
      "Epoch 268/500\n",
      "17500/17500 [==============================] - 1s 52us/step - loss: 0.1272 - acc: 0.9567 - val_loss: 0.3768 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00268: val_acc did not improve from 0.87467\n",
      "Epoch 269/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1270 - acc: 0.9559 - val_loss: 0.3775 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00269: val_acc did not improve from 0.87467\n",
      "Epoch 270/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.1265 - acc: 0.9563 - val_loss: 0.3782 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00270: val_acc did not improve from 0.87467\n",
      "Epoch 271/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1263 - acc: 0.9570 - val_loss: 0.3787 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00271: val_acc did not improve from 0.87467\n",
      "Epoch 272/500\n",
      "17500/17500 [==============================] - 1s 53us/step - loss: 0.1250 - acc: 0.9558 - val_loss: 0.3793 - val_acc: 0.8564\n",
      "\n",
      "Epoch 00272: val_acc did not improve from 0.87467\n",
      "Epoch 273/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1265 - acc: 0.9573 - val_loss: 0.3800 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00273: val_acc did not improve from 0.87467\n",
      "Epoch 274/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1251 - acc: 0.9570 - val_loss: 0.3807 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00274: val_acc did not improve from 0.87467\n",
      "Epoch 275/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1249 - acc: 0.9557 - val_loss: 0.3814 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00275: val_acc did not improve from 0.87467\n",
      "Epoch 276/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1239 - acc: 0.9577 - val_loss: 0.3822 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00276: val_acc did not improve from 0.87467\n",
      "Epoch 277/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1233 - acc: 0.9581 - val_loss: 0.3831 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00277: val_acc did not improve from 0.87467\n",
      "Epoch 278/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1238 - acc: 0.9570 - val_loss: 0.3839 - val_acc: 0.8556\n",
      "\n",
      "Epoch 00278: val_acc did not improve from 0.87467\n",
      "Epoch 279/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1223 - acc: 0.9574 - val_loss: 0.3845 - val_acc: 0.8552\n",
      "\n",
      "Epoch 00279: val_acc did not improve from 0.87467\n",
      "Epoch 280/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1223 - acc: 0.9578 - val_loss: 0.3852 - val_acc: 0.8549\n",
      "\n",
      "Epoch 00280: val_acc did not improve from 0.87467\n",
      "Epoch 281/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1236 - acc: 0.9562 - val_loss: 0.3858 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00281: val_acc did not improve from 0.87467\n",
      "Epoch 282/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1230 - acc: 0.9581 - val_loss: 0.3863 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00282: val_acc did not improve from 0.87467\n",
      "Epoch 283/500\n",
      "17500/17500 [==============================] - 1s 53us/step - loss: 0.1230 - acc: 0.9569 - val_loss: 0.3869 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00283: val_acc did not improve from 0.87467\n",
      "Epoch 284/500\n",
      "17500/17500 [==============================] - 1s 51us/step - loss: 0.1223 - acc: 0.9561 - val_loss: 0.3875 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00284: val_acc did not improve from 0.87467\n",
      "Epoch 285/500\n",
      "17500/17500 [==============================] - 1s 58us/step - loss: 0.1206 - acc: 0.9579 - val_loss: 0.3882 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00285: val_acc did not improve from 0.87467\n",
      "Epoch 286/500\n",
      "17500/17500 [==============================] - 1s 54us/step - loss: 0.1211 - acc: 0.9587 - val_loss: 0.3889 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00286: val_acc did not improve from 0.87467\n",
      "Epoch 287/500\n",
      "17500/17500 [==============================] - 1s 69us/step - loss: 0.1205 - acc: 0.9606 - val_loss: 0.3896 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00287: val_acc did not improve from 0.87467\n",
      "Epoch 288/500\n",
      "17500/17500 [==============================] - 1s 62us/step - loss: 0.1200 - acc: 0.9586 - val_loss: 0.3904 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00288: val_acc did not improve from 0.87467\n",
      "Epoch 289/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1203 - acc: 0.9606 - val_loss: 0.3912 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00289: val_acc did not improve from 0.87467\n",
      "Epoch 290/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1204 - acc: 0.9581 - val_loss: 0.3918 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00290: val_acc did not improve from 0.87467\n",
      "Epoch 291/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1206 - acc: 0.9593 - val_loss: 0.3924 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00291: val_acc did not improve from 0.87467\n",
      "Epoch 292/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1198 - acc: 0.9594 - val_loss: 0.3930 - val_acc: 0.8548\n",
      "\n",
      "Epoch 00292: val_acc did not improve from 0.87467\n",
      "Epoch 293/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1178 - acc: 0.9594 - val_loss: 0.3939 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00293: val_acc did not improve from 0.87467\n",
      "Epoch 294/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1191 - acc: 0.9584 - val_loss: 0.3947 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00294: val_acc did not improve from 0.87467\n",
      "Epoch 295/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1179 - acc: 0.9602 - val_loss: 0.3954 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00295: val_acc did not improve from 0.87467\n",
      "Epoch 296/500\n",
      "17500/17500 [==============================] - 1s 51us/step - loss: 0.1186 - acc: 0.9597 - val_loss: 0.3961 - val_acc: 0.8543\n",
      "\n",
      "Epoch 00296: val_acc did not improve from 0.87467\n",
      "Epoch 297/500\n",
      "17500/17500 [==============================] - 1s 49us/step - loss: 0.1177 - acc: 0.9591 - val_loss: 0.3968 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00297: val_acc did not improve from 0.87467\n",
      "Epoch 298/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1162 - acc: 0.9605 - val_loss: 0.3976 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00298: val_acc did not improve from 0.87467\n",
      "Epoch 299/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1172 - acc: 0.9602 - val_loss: 0.3984 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00299: val_acc did not improve from 0.87467\n",
      "Epoch 300/500\n",
      "17500/17500 [==============================] - 1s 50us/step - loss: 0.1166 - acc: 0.9607 - val_loss: 0.3990 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00300: val_acc did not improve from 0.87467\n",
      "Epoch 301/500\n",
      "17500/17500 [==============================] - 1s 52us/step - loss: 0.1171 - acc: 0.9607 - val_loss: 0.3997 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00301: val_acc did not improve from 0.87467\n",
      "Epoch 302/500\n",
      "17500/17500 [==============================] - 1s 46us/step - loss: 0.1170 - acc: 0.9606 - val_loss: 0.4004 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00302: val_acc did not improve from 0.87467\n",
      "Epoch 303/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1150 - acc: 0.9622 - val_loss: 0.4013 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00303: val_acc did not improve from 0.87467\n",
      "Epoch 304/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1150 - acc: 0.9624 - val_loss: 0.4022 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00304: val_acc did not improve from 0.87467\n",
      "Epoch 305/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1160 - acc: 0.9617 - val_loss: 0.4030 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00305: val_acc did not improve from 0.87467\n",
      "Epoch 306/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1151 - acc: 0.9618 - val_loss: 0.4036 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00306: val_acc did not improve from 0.87467\n",
      "Epoch 307/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1146 - acc: 0.9607 - val_loss: 0.4042 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00307: val_acc did not improve from 0.87467\n",
      "Epoch 308/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1149 - acc: 0.9605 - val_loss: 0.4048 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00308: val_acc did not improve from 0.87467\n",
      "Epoch 309/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1155 - acc: 0.9599 - val_loss: 0.4054 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00309: val_acc did not improve from 0.87467\n",
      "Epoch 310/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1147 - acc: 0.9611 - val_loss: 0.4061 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00310: val_acc did not improve from 0.87467\n",
      "Epoch 311/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1139 - acc: 0.9628 - val_loss: 0.4068 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00311: val_acc did not improve from 0.87467\n",
      "Epoch 312/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1140 - acc: 0.9598 - val_loss: 0.4076 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00312: val_acc did not improve from 0.87467\n",
      "Epoch 313/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1133 - acc: 0.9625 - val_loss: 0.4084 - val_acc: 0.8533\n",
      "\n",
      "Epoch 00313: val_acc did not improve from 0.87467\n",
      "Epoch 314/500\n",
      "17500/17500 [==============================] - 1s 45us/step - loss: 0.1124 - acc: 0.9615 - val_loss: 0.4090 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00314: val_acc did not improve from 0.87467\n",
      "Epoch 315/500\n",
      "17500/17500 [==============================] - 1s 44us/step - loss: 0.1124 - acc: 0.9620 - val_loss: 0.4097 - val_acc: 0.8528\n",
      "\n",
      "Epoch 00315: val_acc did not improve from 0.87467\n",
      "Epoch 316/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1124 - acc: 0.9627 - val_loss: 0.4105 - val_acc: 0.8531\n",
      "\n",
      "Epoch 00316: val_acc did not improve from 0.87467\n",
      "Epoch 317/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1125 - acc: 0.9620 - val_loss: 0.4112 - val_acc: 0.8532\n",
      "\n",
      "Epoch 00317: val_acc did not improve from 0.87467\n",
      "Epoch 318/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1134 - acc: 0.9630 - val_loss: 0.4119 - val_acc: 0.8529\n",
      "\n",
      "Epoch 00318: val_acc did not improve from 0.87467\n",
      "Epoch 319/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1132 - acc: 0.9618 - val_loss: 0.4125 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00319: val_acc did not improve from 0.87467\n",
      "Epoch 320/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1104 - acc: 0.9632 - val_loss: 0.4132 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00320: val_acc did not improve from 0.87467\n",
      "Epoch 321/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1125 - acc: 0.9602 - val_loss: 0.4138 - val_acc: 0.8525\n",
      "\n",
      "Epoch 00321: val_acc did not improve from 0.87467\n",
      "Epoch 322/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1105 - acc: 0.9622 - val_loss: 0.4145 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00322: val_acc did not improve from 0.87467\n",
      "Epoch 323/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1103 - acc: 0.9640 - val_loss: 0.4152 - val_acc: 0.8524\n",
      "\n",
      "Epoch 00323: val_acc did not improve from 0.87467\n",
      "Epoch 324/500\n",
      "17500/17500 [==============================] - 1s 43us/step - loss: 0.1102 - acc: 0.9622 - val_loss: 0.4162 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00324: val_acc did not improve from 0.87467\n",
      "Epoch 325/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.1091 - acc: 0.9626 - val_loss: 0.4171 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00325: val_acc did not improve from 0.87467\n",
      "Epoch 326/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1103 - acc: 0.9637 - val_loss: 0.4180 - val_acc: 0.8523\n",
      "\n",
      "Epoch 00326: val_acc did not improve from 0.87467\n",
      "Epoch 327/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1108 - acc: 0.9622 - val_loss: 0.4186 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00327: val_acc did not improve from 0.87467\n",
      "Epoch 328/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1092 - acc: 0.9626 - val_loss: 0.4191 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00328: val_acc did not improve from 0.87467\n",
      "Epoch 329/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1085 - acc: 0.9634 - val_loss: 0.4198 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00329: val_acc did not improve from 0.87467\n",
      "Epoch 330/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1078 - acc: 0.9649 - val_loss: 0.4208 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00330: val_acc did not improve from 0.87467\n",
      "Epoch 331/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1072 - acc: 0.9647 - val_loss: 0.4217 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00331: val_acc did not improve from 0.87467\n",
      "Epoch 332/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1084 - acc: 0.9630 - val_loss: 0.4226 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00332: val_acc did not improve from 0.87467\n",
      "Epoch 333/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1100 - acc: 0.9634 - val_loss: 0.4231 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00333: val_acc did not improve from 0.87467\n",
      "Epoch 334/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1083 - acc: 0.9623 - val_loss: 0.4234 - val_acc: 0.8519\n",
      "\n",
      "Epoch 00334: val_acc did not improve from 0.87467\n",
      "Epoch 335/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1072 - acc: 0.9643 - val_loss: 0.4239 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00335: val_acc did not improve from 0.87467\n",
      "Epoch 336/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1079 - acc: 0.9645 - val_loss: 0.4246 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00336: val_acc did not improve from 0.87467\n",
      "Epoch 337/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1076 - acc: 0.9632 - val_loss: 0.4253 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00337: val_acc did not improve from 0.87467\n",
      "Epoch 338/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1063 - acc: 0.9643 - val_loss: 0.4262 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00338: val_acc did not improve from 0.87467\n",
      "Epoch 339/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1053 - acc: 0.9647 - val_loss: 0.4272 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00339: val_acc did not improve from 0.87467\n",
      "Epoch 340/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1057 - acc: 0.9653 - val_loss: 0.4281 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00340: val_acc did not improve from 0.87467\n",
      "Epoch 341/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1061 - acc: 0.9646 - val_loss: 0.4288 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00341: val_acc did not improve from 0.87467\n",
      "Epoch 342/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1050 - acc: 0.9653 - val_loss: 0.4294 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00342: val_acc did not improve from 0.87467\n",
      "Epoch 343/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1045 - acc: 0.9644 - val_loss: 0.4300 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00343: val_acc did not improve from 0.87467\n",
      "Epoch 344/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1065 - acc: 0.9633 - val_loss: 0.4306 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00344: val_acc did not improve from 0.87467\n",
      "Epoch 345/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1066 - acc: 0.9650 - val_loss: 0.4310 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00345: val_acc did not improve from 0.87467\n",
      "Epoch 346/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1045 - acc: 0.9652 - val_loss: 0.4317 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00346: val_acc did not improve from 0.87467\n",
      "Epoch 347/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1051 - acc: 0.9638 - val_loss: 0.4324 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00347: val_acc did not improve from 0.87467\n",
      "Epoch 348/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1049 - acc: 0.9643 - val_loss: 0.4331 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00348: val_acc did not improve from 0.87467\n",
      "Epoch 349/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1053 - acc: 0.9659 - val_loss: 0.4339 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00349: val_acc did not improve from 0.87467\n",
      "Epoch 350/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1031 - acc: 0.9665 - val_loss: 0.4345 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00350: val_acc did not improve from 0.87467\n",
      "Epoch 351/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1032 - acc: 0.9663 - val_loss: 0.4354 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00351: val_acc did not improve from 0.87467\n",
      "Epoch 352/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1019 - acc: 0.9670 - val_loss: 0.4364 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00352: val_acc did not improve from 0.87467\n",
      "Epoch 353/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1030 - acc: 0.9670 - val_loss: 0.4371 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00353: val_acc did not improve from 0.87467\n",
      "Epoch 354/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1021 - acc: 0.9655 - val_loss: 0.4376 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00354: val_acc did not improve from 0.87467\n",
      "Epoch 355/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1022 - acc: 0.9669 - val_loss: 0.4383 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00355: val_acc did not improve from 0.87467\n",
      "Epoch 356/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1028 - acc: 0.9653 - val_loss: 0.4389 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00356: val_acc did not improve from 0.87467\n",
      "Epoch 357/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1021 - acc: 0.9657 - val_loss: 0.4397 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00357: val_acc did not improve from 0.87467\n",
      "Epoch 358/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1020 - acc: 0.9664 - val_loss: 0.4406 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00358: val_acc did not improve from 0.87467\n",
      "Epoch 359/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1006 - acc: 0.9668 - val_loss: 0.4413 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00359: val_acc did not improve from 0.87467\n",
      "Epoch 360/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1020 - acc: 0.9660 - val_loss: 0.4421 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00360: val_acc did not improve from 0.87467\n",
      "Epoch 361/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1002 - acc: 0.9676 - val_loss: 0.4429 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00361: val_acc did not improve from 0.87467\n",
      "Epoch 362/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1018 - acc: 0.9655 - val_loss: 0.4436 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00362: val_acc did not improve from 0.87467\n",
      "Epoch 363/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1006 - acc: 0.9658 - val_loss: 0.4440 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00363: val_acc did not improve from 0.87467\n",
      "Epoch 364/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0993 - acc: 0.9682 - val_loss: 0.4446 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00364: val_acc did not improve from 0.87467\n",
      "Epoch 365/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0983 - acc: 0.9690 - val_loss: 0.4458 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00365: val_acc did not improve from 0.87467\n",
      "Epoch 366/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.1002 - acc: 0.9671 - val_loss: 0.4468 - val_acc: 0.8516\n",
      "\n",
      "Epoch 00366: val_acc did not improve from 0.87467\n",
      "Epoch 367/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0987 - acc: 0.9677 - val_loss: 0.4478 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00367: val_acc did not improve from 0.87467\n",
      "Epoch 368/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.1004 - acc: 0.9670 - val_loss: 0.4484 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00368: val_acc did not improve from 0.87467\n",
      "Epoch 369/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0998 - acc: 0.9672 - val_loss: 0.4489 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00369: val_acc did not improve from 0.87467\n",
      "Epoch 370/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0972 - acc: 0.9683 - val_loss: 0.4495 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00370: val_acc did not improve from 0.87467\n",
      "Epoch 371/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0973 - acc: 0.9677 - val_loss: 0.4502 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00371: val_acc did not improve from 0.87467\n",
      "Epoch 372/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0971 - acc: 0.9679 - val_loss: 0.4513 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00372: val_acc did not improve from 0.87467\n",
      "Epoch 373/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.0985 - acc: 0.9678 - val_loss: 0.4524 - val_acc: 0.8512\n",
      "\n",
      "Epoch 00373: val_acc did not improve from 0.87467\n",
      "Epoch 374/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.0975 - acc: 0.9686 - val_loss: 0.4533 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00374: val_acc did not improve from 0.87467\n",
      "Epoch 375/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0978 - acc: 0.9675 - val_loss: 0.4536 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00375: val_acc did not improve from 0.87467\n",
      "Epoch 376/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0970 - acc: 0.9695 - val_loss: 0.4539 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00376: val_acc did not improve from 0.87467\n",
      "Epoch 377/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0977 - acc: 0.9679 - val_loss: 0.4545 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00377: val_acc did not improve from 0.87467\n",
      "Epoch 378/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0960 - acc: 0.9684 - val_loss: 0.4553 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00378: val_acc did not improve from 0.87467\n",
      "Epoch 379/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0953 - acc: 0.9691 - val_loss: 0.4562 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00379: val_acc did not improve from 0.87467\n",
      "Epoch 380/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0963 - acc: 0.9691 - val_loss: 0.4571 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00380: val_acc did not improve from 0.87467\n",
      "Epoch 381/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0947 - acc: 0.9698 - val_loss: 0.4580 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00381: val_acc did not improve from 0.87467\n",
      "Epoch 382/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0960 - acc: 0.9687 - val_loss: 0.4589 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00382: val_acc did not improve from 0.87467\n",
      "Epoch 383/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0946 - acc: 0.9695 - val_loss: 0.4597 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00383: val_acc did not improve from 0.87467\n",
      "Epoch 384/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0956 - acc: 0.9683 - val_loss: 0.4605 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00384: val_acc did not improve from 0.87467\n",
      "Epoch 385/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0945 - acc: 0.9691 - val_loss: 0.4611 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00385: val_acc did not improve from 0.87467\n",
      "Epoch 386/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0944 - acc: 0.9704 - val_loss: 0.4617 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00386: val_acc did not improve from 0.87467\n",
      "Epoch 387/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0940 - acc: 0.9697 - val_loss: 0.4625 - val_acc: 0.8505\n",
      "\n",
      "Epoch 00387: val_acc did not improve from 0.87467\n",
      "Epoch 388/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0944 - acc: 0.9702 - val_loss: 0.4634 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00388: val_acc did not improve from 0.87467\n",
      "Epoch 389/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.0938 - acc: 0.9689 - val_loss: 0.4640 - val_acc: 0.8511\n",
      "\n",
      "Epoch 00389: val_acc did not improve from 0.87467\n",
      "Epoch 390/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0941 - acc: 0.9692 - val_loss: 0.4646 - val_acc: 0.8509\n",
      "\n",
      "Epoch 00390: val_acc did not improve from 0.87467\n",
      "Epoch 391/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0925 - acc: 0.9704 - val_loss: 0.4654 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00391: val_acc did not improve from 0.87467\n",
      "Epoch 392/500\n",
      "17500/17500 [==============================] - 1s 42us/step - loss: 0.0941 - acc: 0.9701 - val_loss: 0.4662 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00392: val_acc did not improve from 0.87467\n",
      "Epoch 393/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0946 - acc: 0.9690 - val_loss: 0.4667 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00393: val_acc did not improve from 0.87467\n",
      "Epoch 394/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0939 - acc: 0.9705 - val_loss: 0.4675 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00394: val_acc did not improve from 0.87467\n",
      "Epoch 395/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0928 - acc: 0.9710 - val_loss: 0.4683 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00395: val_acc did not improve from 0.87467\n",
      "Epoch 396/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0925 - acc: 0.9702 - val_loss: 0.4694 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00396: val_acc did not improve from 0.87467\n",
      "Epoch 397/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0928 - acc: 0.9706 - val_loss: 0.4702 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00397: val_acc did not improve from 0.87467\n",
      "Epoch 398/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0921 - acc: 0.9715 - val_loss: 0.4709 - val_acc: 0.8501\n",
      "\n",
      "Epoch 00398: val_acc did not improve from 0.87467\n",
      "Epoch 399/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0929 - acc: 0.9691 - val_loss: 0.4716 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00399: val_acc did not improve from 0.87467\n",
      "Epoch 400/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0920 - acc: 0.9705 - val_loss: 0.4721 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00400: val_acc did not improve from 0.87467\n",
      "Epoch 401/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0911 - acc: 0.9718 - val_loss: 0.4730 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00401: val_acc did not improve from 0.87467\n",
      "Epoch 402/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0924 - acc: 0.9703 - val_loss: 0.4737 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00402: val_acc did not improve from 0.87467\n",
      "Epoch 403/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0912 - acc: 0.9704 - val_loss: 0.4746 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00403: val_acc did not improve from 0.87467\n",
      "Epoch 404/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0917 - acc: 0.9713 - val_loss: 0.4755 - val_acc: 0.8499\n",
      "\n",
      "Epoch 00404: val_acc did not improve from 0.87467\n",
      "Epoch 405/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0902 - acc: 0.9708 - val_loss: 0.4763 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00405: val_acc did not improve from 0.87467\n",
      "Epoch 406/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0901 - acc: 0.9711 - val_loss: 0.4772 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00406: val_acc did not improve from 0.87467\n",
      "Epoch 407/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0908 - acc: 0.9707 - val_loss: 0.4780 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00407: val_acc did not improve from 0.87467\n",
      "Epoch 408/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0903 - acc: 0.9699 - val_loss: 0.4784 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00408: val_acc did not improve from 0.87467\n",
      "Epoch 409/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0897 - acc: 0.9724 - val_loss: 0.4791 - val_acc: 0.8496\n",
      "\n",
      "Epoch 00409: val_acc did not improve from 0.87467\n",
      "Epoch 410/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0902 - acc: 0.9716 - val_loss: 0.4800 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00410: val_acc did not improve from 0.87467\n",
      "Epoch 411/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0893 - acc: 0.9715 - val_loss: 0.4807 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00411: val_acc did not improve from 0.87467\n",
      "Epoch 412/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0890 - acc: 0.9725 - val_loss: 0.4817 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00412: val_acc did not improve from 0.87467\n",
      "Epoch 413/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0885 - acc: 0.9730 - val_loss: 0.4826 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00413: val_acc did not improve from 0.87467\n",
      "Epoch 414/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0884 - acc: 0.9729 - val_loss: 0.4831 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00414: val_acc did not improve from 0.87467\n",
      "Epoch 415/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0890 - acc: 0.9722 - val_loss: 0.4838 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00415: val_acc did not improve from 0.87467\n",
      "Epoch 416/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0871 - acc: 0.9723 - val_loss: 0.4849 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00416: val_acc did not improve from 0.87467\n",
      "Epoch 417/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0884 - acc: 0.9717 - val_loss: 0.4859 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00417: val_acc did not improve from 0.87467\n",
      "Epoch 418/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0875 - acc: 0.9731 - val_loss: 0.4868 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00418: val_acc did not improve from 0.87467\n",
      "Epoch 419/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0876 - acc: 0.9711 - val_loss: 0.4875 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00419: val_acc did not improve from 0.87467\n",
      "Epoch 420/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0876 - acc: 0.9726 - val_loss: 0.4882 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00420: val_acc did not improve from 0.87467\n",
      "Epoch 421/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0883 - acc: 0.9726 - val_loss: 0.4886 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00421: val_acc did not improve from 0.87467\n",
      "Epoch 422/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0867 - acc: 0.9738 - val_loss: 0.4891 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00422: val_acc did not improve from 0.87467\n",
      "Epoch 423/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0858 - acc: 0.9730 - val_loss: 0.4901 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00423: val_acc did not improve from 0.87467\n",
      "Epoch 424/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0862 - acc: 0.9737 - val_loss: 0.4915 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00424: val_acc did not improve from 0.87467\n",
      "Epoch 425/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0859 - acc: 0.9742 - val_loss: 0.4924 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00425: val_acc did not improve from 0.87467\n",
      "Epoch 426/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0860 - acc: 0.9744 - val_loss: 0.4929 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00426: val_acc did not improve from 0.87467\n",
      "Epoch 427/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0861 - acc: 0.9737 - val_loss: 0.4936 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00427: val_acc did not improve from 0.87467\n",
      "Epoch 428/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0858 - acc: 0.9730 - val_loss: 0.4941 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00428: val_acc did not improve from 0.87467\n",
      "Epoch 429/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0853 - acc: 0.9745 - val_loss: 0.4948 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00429: val_acc did not improve from 0.87467\n",
      "Epoch 430/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0842 - acc: 0.9743 - val_loss: 0.4957 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00430: val_acc did not improve from 0.87467\n",
      "Epoch 431/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0851 - acc: 0.9736 - val_loss: 0.4967 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00431: val_acc did not improve from 0.87467\n",
      "Epoch 432/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0852 - acc: 0.9741 - val_loss: 0.4975 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00432: val_acc did not improve from 0.87467\n",
      "Epoch 433/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0853 - acc: 0.9737 - val_loss: 0.4979 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00433: val_acc did not improve from 0.87467\n",
      "Epoch 434/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0844 - acc: 0.9754 - val_loss: 0.4986 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00434: val_acc did not improve from 0.87467\n",
      "Epoch 435/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0827 - acc: 0.9756 - val_loss: 0.4997 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00435: val_acc did not improve from 0.87467\n",
      "Epoch 436/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0841 - acc: 0.9742 - val_loss: 0.5007 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00436: val_acc did not improve from 0.87467\n",
      "Epoch 437/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0832 - acc: 0.9739 - val_loss: 0.5012 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00437: val_acc did not improve from 0.87467\n",
      "Epoch 438/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0831 - acc: 0.9737 - val_loss: 0.5014 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00438: val_acc did not improve from 0.87467\n",
      "Epoch 439/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0837 - acc: 0.9744 - val_loss: 0.5019 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00439: val_acc did not improve from 0.87467\n",
      "Epoch 440/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0823 - acc: 0.9758 - val_loss: 0.5030 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00440: val_acc did not improve from 0.87467\n",
      "Epoch 441/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0841 - acc: 0.9738 - val_loss: 0.5040 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00441: val_acc did not improve from 0.87467\n",
      "Epoch 442/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0836 - acc: 0.9745 - val_loss: 0.5045 - val_acc: 0.8493\n",
      "\n",
      "Epoch 00442: val_acc did not improve from 0.87467\n",
      "Epoch 443/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0832 - acc: 0.9741 - val_loss: 0.5049 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00443: val_acc did not improve from 0.87467\n",
      "Epoch 444/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0825 - acc: 0.9745 - val_loss: 0.5054 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00444: val_acc did not improve from 0.87467\n",
      "Epoch 445/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0823 - acc: 0.9754 - val_loss: 0.5064 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00445: val_acc did not improve from 0.87467\n",
      "Epoch 446/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0815 - acc: 0.9753 - val_loss: 0.5075 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00446: val_acc did not improve from 0.87467\n",
      "Epoch 447/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0825 - acc: 0.9738 - val_loss: 0.5083 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00447: val_acc did not improve from 0.87467\n",
      "Epoch 448/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0818 - acc: 0.9751 - val_loss: 0.5089 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00448: val_acc did not improve from 0.87467\n",
      "Epoch 449/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0811 - acc: 0.9758 - val_loss: 0.5096 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00449: val_acc did not improve from 0.87467\n",
      "Epoch 450/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0825 - acc: 0.9747 - val_loss: 0.5105 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00450: val_acc did not improve from 0.87467\n",
      "Epoch 451/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0805 - acc: 0.9757 - val_loss: 0.5114 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00451: val_acc did not improve from 0.87467\n",
      "Epoch 452/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0816 - acc: 0.9747 - val_loss: 0.5122 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00452: val_acc did not improve from 0.87467\n",
      "Epoch 453/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0807 - acc: 0.9750 - val_loss: 0.5128 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00453: val_acc did not improve from 0.87467\n",
      "Epoch 454/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0816 - acc: 0.9754 - val_loss: 0.5133 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00454: val_acc did not improve from 0.87467\n",
      "Epoch 455/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0801 - acc: 0.9758 - val_loss: 0.5141 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00455: val_acc did not improve from 0.87467\n",
      "Epoch 456/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0789 - acc: 0.9767 - val_loss: 0.5153 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00456: val_acc did not improve from 0.87467\n",
      "Epoch 457/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0796 - acc: 0.9766 - val_loss: 0.5164 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00457: val_acc did not improve from 0.87467\n",
      "Epoch 458/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0791 - acc: 0.9774 - val_loss: 0.5171 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00458: val_acc did not improve from 0.87467\n",
      "Epoch 459/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0799 - acc: 0.9764 - val_loss: 0.5173 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00459: val_acc did not improve from 0.87467\n",
      "Epoch 460/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0782 - acc: 0.9778 - val_loss: 0.5182 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00460: val_acc did not improve from 0.87467\n",
      "Epoch 461/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0783 - acc: 0.9774 - val_loss: 0.5195 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00461: val_acc did not improve from 0.87467\n",
      "Epoch 462/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0790 - acc: 0.9774 - val_loss: 0.5206 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00462: val_acc did not improve from 0.87467\n",
      "Epoch 463/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0781 - acc: 0.9767 - val_loss: 0.5214 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00463: val_acc did not improve from 0.87467\n",
      "Epoch 464/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0786 - acc: 0.9758 - val_loss: 0.5219 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00464: val_acc did not improve from 0.87467\n",
      "Epoch 465/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0785 - acc: 0.9757 - val_loss: 0.5225 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00465: val_acc did not improve from 0.87467\n",
      "Epoch 466/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0774 - acc: 0.9770 - val_loss: 0.5233 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00466: val_acc did not improve from 0.87467\n",
      "Epoch 467/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0786 - acc: 0.9766 - val_loss: 0.5241 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00467: val_acc did not improve from 0.87467\n",
      "Epoch 468/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0768 - acc: 0.9782 - val_loss: 0.5254 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00468: val_acc did not improve from 0.87467\n",
      "Epoch 469/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0771 - acc: 0.9775 - val_loss: 0.5267 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00469: val_acc did not improve from 0.87467\n",
      "Epoch 470/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0766 - acc: 0.9773 - val_loss: 0.5274 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00470: val_acc did not improve from 0.87467\n",
      "Epoch 471/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0782 - acc: 0.9779 - val_loss: 0.5275 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00471: val_acc did not improve from 0.87467\n",
      "Epoch 472/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0761 - acc: 0.9785 - val_loss: 0.5279 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00472: val_acc did not improve from 0.87467\n",
      "Epoch 473/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0768 - acc: 0.9774 - val_loss: 0.5286 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00473: val_acc did not improve from 0.87467\n",
      "Epoch 474/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0752 - acc: 0.9793 - val_loss: 0.5299 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00474: val_acc did not improve from 0.87467\n",
      "Epoch 475/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0763 - acc: 0.9792 - val_loss: 0.5312 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00475: val_acc did not improve from 0.87467\n",
      "Epoch 476/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0760 - acc: 0.9791 - val_loss: 0.5320 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00476: val_acc did not improve from 0.87467\n",
      "Epoch 477/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0777 - acc: 0.9767 - val_loss: 0.5326 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00477: val_acc did not improve from 0.87467\n",
      "Epoch 478/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0758 - acc: 0.9786 - val_loss: 0.5330 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00478: val_acc did not improve from 0.87467\n",
      "Epoch 479/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0750 - acc: 0.9784 - val_loss: 0.5337 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00479: val_acc did not improve from 0.87467\n",
      "Epoch 480/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0768 - acc: 0.9783 - val_loss: 0.5345 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00480: val_acc did not improve from 0.87467\n",
      "Epoch 481/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0751 - acc: 0.9788 - val_loss: 0.5352 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00481: val_acc did not improve from 0.87467\n",
      "Epoch 482/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0746 - acc: 0.9787 - val_loss: 0.5361 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00482: val_acc did not improve from 0.87467\n",
      "Epoch 483/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0747 - acc: 0.9795 - val_loss: 0.5370 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00483: val_acc did not improve from 0.87467\n",
      "Epoch 484/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0743 - acc: 0.9790 - val_loss: 0.5377 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00484: val_acc did not improve from 0.87467\n",
      "Epoch 485/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0759 - acc: 0.9784 - val_loss: 0.5383 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00485: val_acc did not improve from 0.87467\n",
      "Epoch 486/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0747 - acc: 0.9786 - val_loss: 0.5391 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00486: val_acc did not improve from 0.87467\n",
      "Epoch 487/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0738 - acc: 0.9793 - val_loss: 0.5402 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00487: val_acc did not improve from 0.87467\n",
      "Epoch 488/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0735 - acc: 0.9797 - val_loss: 0.5415 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00488: val_acc did not improve from 0.87467\n",
      "Epoch 489/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0735 - acc: 0.9803 - val_loss: 0.5421 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00489: val_acc did not improve from 0.87467\n",
      "Epoch 490/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0729 - acc: 0.9801 - val_loss: 0.5424 - val_acc: 0.8484\n",
      "\n",
      "Epoch 00490: val_acc did not improve from 0.87467\n",
      "Epoch 491/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0731 - acc: 0.9795 - val_loss: 0.5434 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00491: val_acc did not improve from 0.87467\n",
      "Epoch 492/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0740 - acc: 0.9787 - val_loss: 0.5446 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00492: val_acc did not improve from 0.87467\n",
      "Epoch 493/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0732 - acc: 0.9795 - val_loss: 0.5456 - val_acc: 0.8487\n",
      "\n",
      "Epoch 00493: val_acc did not improve from 0.87467\n",
      "Epoch 494/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0729 - acc: 0.9798 - val_loss: 0.5464 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00494: val_acc did not improve from 0.87467\n",
      "Epoch 495/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0719 - acc: 0.9805 - val_loss: 0.5470 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00495: val_acc did not improve from 0.87467\n",
      "Epoch 496/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0717 - acc: 0.9816 - val_loss: 0.5477 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00496: val_acc did not improve from 0.87467\n",
      "Epoch 497/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0724 - acc: 0.9802 - val_loss: 0.5484 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00497: val_acc did not improve from 0.87467\n",
      "Epoch 498/500\n",
      "17500/17500 [==============================] - 1s 41us/step - loss: 0.0713 - acc: 0.9809 - val_loss: 0.5495 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00498: val_acc did not improve from 0.87467\n",
      "Epoch 499/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0721 - acc: 0.9797 - val_loss: 0.5502 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00499: val_acc did not improve from 0.87467\n",
      "Epoch 500/500\n",
      "17500/17500 [==============================] - 1s 40us/step - loss: 0.0710 - acc: 0.9811 - val_loss: 0.5509 - val_acc: 0.8480\n",
      "\n",
      "Epoch 00500: val_acc did not improve from 0.87467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x47b50828>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#None 维度在这里是一个 batch size 的占位符\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train test split\n",
    "mlp_train_x, mlp_test_x, mlp_train_y, mlp_test_y = train_test_split(tfidf_train_x, label, test_size=0.3, random_state=123)\n",
    "\n",
    "model_MLP = Sequential()\n",
    "#model.add(Dense(3, activation='relu', input_shape=(18,)))\n",
    "\n",
    "model_MLP.add(Dense(10, input_shape=(4000,), activation='relu'))#\n",
    "\n",
    "model_MLP.add(Dropout(0.2))\n",
    "model_MLP.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_MLP.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_MLP.summary()\n",
    "\n",
    "keras_y_train = np.array(keras.utils.to_categorical(mlp_train_y, 2))\n",
    "keras_y_test = np.array(keras.utils.to_categorical(mlp_test_y, 2))\n",
    "\n",
    "#仅保存最好的模型\n",
    "filepath=\"model_MLP/weights.best.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')#验证集准确率比之前效果好就保存权重\n",
    "\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model_MLP.fit(mlp_train_x, keras_y_train, validation_data=(mlp_test_x, keras_y_test), epochs=500, batch_size=5000,callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 10)                40010     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 40,032\n",
      "Trainable params: 40,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "model_MLP = Sequential()\n",
    "\n",
    "model_MLP.add(Dense(10, input_shape=(4000,), activation='relu'))#\n",
    "\n",
    "model_MLP.add(Dropout(0.2))\n",
    "model_MLP.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model_MLP.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_MLP.summary()\n",
    "\n",
    "# load weights\n",
    "model_MLP.load_weights(\"model_MLP/weights.best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "test_predicted = np.array(model_MLP.predict_classes(tfidf_test_x))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_mlp_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. TF-IDF+DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "决策树 5折交叉验证得分: \n",
      " [0.7128 0.705  0.7102 0.7062 0.7096]\n",
      "决策树 5折交叉验证平均得分: \n",
      " 0.7087600000000001\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_DT = DecisionTreeClassifier()\n",
    "model_DT.fit(tfidf_train_x, label)\n",
    "\n",
    "scores = cross_val_score(model_DT, tfidf_train_x, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"决策树 5折交叉验证得分: \", scores)\n",
    "print(\"决策树 5折交叉验证平均得分: \", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(model_DT.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_dt_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. TF-IDF+xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB 5折交叉验证得分:  [0.91607008 0.91845504 0.91138704 0.922242   0.9142976 ]\n",
      "XGB 5折交叉验证平均得分:  0.916490352\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model_XGB = XGBClassifier(n_estimators=150, min_samples_leaf=3, max_depth=6)\n",
    "\"\"\"\n",
    "AttributeError: 'list' object has no attribute 'shape'\n",
    "list => np.array\n",
    "\"\"\"\n",
    "model_XGB.fit(tfidf_train_x, label)\n",
    "\n",
    "scores = cross_val_score(model_XGB, tfidf_train_x, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"XGB 5折交叉验证得分: \", scores)\n",
    "print(\"XGB 5折交叉验证平均得分: \", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(model_XGB.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_xgb_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. TF-IDF+GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT 5折交叉验证得分: \n",
      " [0.88863632 0.88981456 0.88274104 0.89644848 0.88723104]\n",
      "GBDT 折交叉验证平均得分: \n",
      " 0.888974288\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_GBDT = GradientBoostingClassifier()\n",
    "model_GBDT.fit(tfidf_train_x, label)\n",
    "\n",
    "scores = cross_val_score(model_GBDT, tfidf_train_x, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"GBDT 5折交叉验证得分: \\n\", scores)\n",
    "print(\"GBDT 折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(model_GBDT.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_gbdt_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. TF-IDF+RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机森林 5折交叉验证得分:  [0.89770248 0.90205984 0.89408512 0.9035528  0.89733304]\n",
      "随机森林 5折交叉验证平均得分:  0.898946656\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_RF = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=0)\n",
    "model_RF.fit(tfidf_train_x, label)\n",
    "\n",
    "scores = cross_val_score(model_RF, tfidf_train_x, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"随机森林 5折交叉验证得分: \", scores)\n",
    "print(\"随机森林 5折交叉验证平均得分: \", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(model_RF.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_rfc_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. TF-IDF+Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 5折交叉验证得分: \n",
      " [0.8394 0.8568 0.8388 0.854  0.8464]\n",
      "VotingClassifier 5折交叉验证平均得分: \n",
      " 0.84708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model_VOT = VotingClassifier(estimators=[('lr', model_LR), ('xgb', model_XGB), ('rf', model_RF)],voting='hard')\n",
    "model_VOT.fit(tfidf_train_x, np.array(label))\n",
    "\n",
    "scores = cross_val_score(model_VOT, tfidf_train_x,label, cv=5, scoring=None,n_jobs = -1)\n",
    "\n",
    "print(\"VotingClassifier 5折交叉验证得分: \\n\", scores)\n",
    "print(\"VotingClassifier 5折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(model_VOT.predict(tfidf_test_x))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_vot_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. TF-IDF+Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "stacking auc 0.9520484800000001\n",
      "stacking auc 0.94503472\n",
      "stacking auc 0.94511664\n",
      "stacking auc 0.9553464\n",
      "stacking auc 0.94973888\n",
      "0.949457024\n",
      "0.1\n",
      "stacking auc 0.95298912\n",
      "stacking auc 0.9471843200000001\n",
      "stacking auc 0.9471433599999999\n",
      "stacking auc 0.9565649600000001\n",
      "stacking auc 0.9505606400000002\n",
      "0.95088848\n",
      "1\n",
      "stacking auc 0.9529368000000001\n",
      "stacking auc 0.9477648\n",
      "stacking auc 0.94768992\n",
      "stacking auc 0.9566342399999999\n",
      "stacking auc 0.95043744\n",
      "0.9510926399999999\n",
      "10\n",
      "stacking auc 0.9528793600000001\n",
      "stacking auc 0.9478297600000001\n",
      "stacking auc 0.94773648\n",
      "stacking auc 0.9566390399999998\n",
      "stacking auc 0.9503939199999999\n",
      "0.9510957120000001\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "'''模型融合中使用到的各个单模型'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# 划分train数据集,调用代码,把数据集名字转成和代码一样\n",
    "X = tfidf_train_x\n",
    "y = np.array(label)\n",
    "\n",
    "X_test_features = tfidf_test_x\n",
    "\n",
    "stacking_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "\n",
    "stacking_xgb = XGBClassifier(n_estimators=150, min_samples_leaf=3, max_depth=6)\n",
    "\n",
    "stacking_rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=0)\n",
    "\n",
    "\n",
    "clfs = [stacking_LR,stacking_xgb,stacking_rf]#模型\n",
    "\n",
    "# 创建n_folds\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=1)#K-Flod数据\n",
    "\n",
    "# 创建零矩阵（存储第一层的预测结果）\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))#行数：训练集的行数，列数：模型的个数\n",
    "\n",
    "dataset_blend_test = np.zeros((X_test_features.shape[0], len(clfs)))#行数：测试集数量，列数：模型的个数\n",
    "dataset_blend_test_j = np.zeros((X_test_features.shape[0], n_folds))#行数：测试集数量，列数：K折\n",
    "\n",
    "# 建立第一层模型\n",
    "for j, clf in enumerate(clfs):#枚举分类器\n",
    "    i = 0\n",
    "    for train_index, test_index in skf.split(X, y):#K折数据\n",
    "        X_1_train, y_1_train, X_1_test, y_1_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_1_train, y_1_train)#第j个模型预测第k折数据\n",
    "        \n",
    "        y_submission = clf.predict_proba(X_1_test)[:, 1]#第j个模型预测剩下的1折数据，去除答案是1的概率列\n",
    "        \n",
    "        dataset_blend_train[test_index, j] = y_submission#第j个模型预测的第k折数据的答案写到预测结果里\n",
    "        \n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_test_features)[:, 1]#对测试集进行预测\n",
    "        \n",
    "        i = i + 1 #第i折\n",
    "        \n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1) #每个模型的K折的预测值取平均做为第j个分类器的预测值\n",
    "    \n",
    "# 用建立第二层模型\n",
    "\n",
    "C = [0.01,0.1,1,10]\n",
    "\n",
    "for i in C:\n",
    "    stacking_model_lr = LR(C=i, max_iter=100)\n",
    "    print(i)\n",
    "    aucs = []\n",
    "    for train_index, test_index in skf.split(dataset_blend_train, y):#K折数据\n",
    "        X_2_train, y_2_train, X_2_test, y_2_test = dataset_blend_train[train_index], y[train_index], dataset_blend_train[test_index], y[test_index]\n",
    "        stacking_model_lr.fit(X_2_train, y_2_train)\n",
    "        test_predict_proba = stacking_model_lr.predict_proba(X_2_test)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_2_test, test_predict_proba, pos_label=1)\n",
    "        print(\"stacking auc\",auc(fpr, tpr))\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    print(np.average(aucs))\n",
    "\n",
    "    stacking_model_lr = LR(C=10, max_iter=100)\n",
    "\n",
    "stacking_model_lr.fit(dataset_blend_train, y)\n",
    "\n",
    "test_predict = stacking_model_lr.predict(dataset_blend_test)\n",
    "\n",
    "test_predicted = np.array(test_predict)\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_stacking_tfidf.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Word2vec+机器学习建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Word2vec+LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR分类器 10折交叉验证得分: \n",
      " [0.9412576  0.93986304 0.94739904 0.93963392 0.93683136 0.93972416\n",
      " 0.94117184 0.94543232 0.93660928 0.94397184]\n",
      "LR分类器 10折交叉验证平均得分: \n",
      " 0.9411894399999999\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "wv_model_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "wv_model_LR.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_model_LR, train_data_features, label, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(\"LR分类器 10折交叉验证得分: \\n\", scores)\n",
    "print(\"LR分类器 10折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_model_LR.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_lr_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Word2vec+GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "高斯贝叶斯分类器 10折交叉验证得分: \n",
      " [0.80043648 0.78343712 0.78856768 0.79913536 0.7964928  0.78470464\n",
      " 0.79811424 0.80671424 0.7928432  0.80504768]\n",
      "\n",
      "高斯贝叶斯分类器 10折交叉验证平均得分: \n",
      " 0.7955493440000001\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          0\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "wv_gnb_model = GNB()\n",
    "wv_gnb_model.fit(train_data_features, label)\n",
    "\n",
    "\n",
    "scores = cross_val_score(wv_gnb_model, train_data_features, label, cv=10, scoring='roc_auc')\n",
    "print(\"\\n高斯贝叶斯分类器 10折交叉验证得分: \\n\", scores)\n",
    "print(\"\\n高斯贝叶斯分类器 10折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_gnb_model.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_gnb_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Word2vec+Knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "knn算法 10折交叉验证得分: \n",
      " [0.89709856 0.89423168 0.90665152 0.8890768  0.89263712 0.89179264\n",
      " 0.88568    0.90213216 0.89707968 0.88941376]\n",
      "\n",
      "knn算法 10折交叉验证平均得分: \n",
      " 0.894579392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "wv_knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "wv_knn_model.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_knn_model, train_data_features, label, cv=10, scoring='roc_auc')\n",
    "\n",
    "print(\"\\nknn算法 10折交叉验证得分: \\n\", scores)\n",
    "print(\"\\nknn算法 10折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_knn_model.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_knn_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Word2vec+SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最好的参数：\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SVC' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-5591ac426418>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 输出结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"最好的参数：\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mwv_svm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"最好的得分：\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "'''\n",
    "线性的SVM只需要，只需要调优正则化参数C\n",
    "基于RBF核的SVM，需要调优gamma参数和C\n",
    "'''\n",
    "\n",
    "wv_svm_model = SVC(kernel='linear',C=10,gamma = 1)\n",
    "wv_svm_model.fit(train_data_features, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "test_predicted = np.array(wv_svm_model.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_svm_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Word2vec+DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "决策树 10折交叉验证得分: \n",
      " [0.7498 0.7506 0.7572 0.7648 0.736 ]\n",
      "\n",
      "决策树 10折交叉验证平均得分: \n",
      " 0.75168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "wv_tree_model = DecisionTreeClassifier()\n",
    "wv_tree_model.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_tree_model, train_data_features, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"决策树 10折交叉验证得分: \\n\", scores)\n",
    "print(\"决策树 10折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_tree_model.predict(test_data_features))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_dtc_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Word2vec+xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB 5折交叉验证得分: \n",
      " [0.94458    0.94526208 0.94390896 0.9463752  0.94129232]\n",
      "XGB 5折交叉验证平均得分: \n",
      " 0.944283712\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "wv_xgb_model = XGBClassifier(n_estimators=150, min_samples_leaf=3, max_depth=6)\n",
    "\"\"\"\n",
    "AttributeError: 'list' object has no attribute 'shape'\n",
    "list => np.array\n",
    "\"\"\"\n",
    "wv_xgb_model.fit(pd.DataFrame(train_data_features), label)\n",
    "\n",
    "scores = cross_val_score(wv_xgb_model, pd.DataFrame(train_data_features), label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"XGB 5折交叉验证得分: \\n\", scores)\n",
    "print(\"XGB 5折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_xgb_model.predict(pd.DataFrame(test_data_features)))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_xgb_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Word2vec+RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "随机森林 5折交叉验证得分: \n",
      " [0.9182096  0.91855744 0.91677872 0.92213184 0.91022928]\n",
      "\n",
      "随机森林 5折交叉验证平均得分: \n",
      " 0.9171813759999999\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "wv_rf_model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=0)\n",
    "wv_rf_model.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_rf_model, train_data_features, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"\\n随机森林 5折交叉验证得分: \\n\", scores)\n",
    "print(\"\\n随机森林 5折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_rf_model.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_rf_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Word2vec+GBDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT 5折交叉验证得分: \n",
      " [0.9328144  0.93499104 0.93023024 0.93560096 0.92776608]\n",
      "GBDT 折交叉验证平均得分: \n",
      " 0.9322805439999999\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "wv_gbdt_model = GradientBoostingClassifier()\n",
    "wv_gbdt_model.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_gbdt_model, train_data_features, label, cv=5, scoring='roc_auc')\n",
    "\n",
    "print(\"GBDT 5折交叉验证得分: \\n\", scores)\n",
    "print(\"GBDT 折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_gbdt_model.predict(test_data_features))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_gbdt_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.Word2vec+Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* adaboost模型训练太耗时,没跑完"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "wv_ab_model = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=2),\n",
    "    n_estimators=600,\n",
    "    learning_rate=1)\n",
    "\n",
    "wv_ab_model.fit(train_data_features, label)\n",
    "\n",
    "scores = cross_val_score(wv_ab_model, train_data_features, label, cv=3, scoring='roc_auc')\n",
    "\n",
    "print(\"AdaBoost 3折交叉验证得分: \\n\", scores)\n",
    "print(\"AdaBoost 3折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_ab_model.predict(test_data_features))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_adaboost_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.Word2vec+Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 5折交叉验证得分: \n",
      " [0.8656 0.8702 0.8644 0.8674 0.8572]\n",
      "VotingClassifier 5折交叉验证平均得分: \n",
      " 0.86496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "wv_vot_model = VotingClassifier(estimators=[('lr', wv_model_LR), ('xgb', wv_xgb_model), ('gbdt', wv_gbdt_model),('rf', wv_rf_model)],voting='hard')\n",
    "wv_vot_model.fit(pd.DataFrame(train_data_features), np.array(label))\n",
    "\n",
    "scores = cross_val_score(wv_vot_model, pd.DataFrame(train_data_features),label, cv=5, scoring=None,n_jobs = -1)\n",
    "\n",
    "print(\"VotingClassifier 5折交叉验证得分: \\n\", scores)\n",
    "print(\"VotingClassifier 5折交叉验证平均得分: \\n\", np.mean(scores))\n",
    "\n",
    "test_predicted = np.array(wv_vot_model.predict(pd.DataFrame(test_data_features)))\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_vot_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.Word2vec+Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1 2 4 5] TEST: [0 3]\n",
      "TRAIN: [0 1 3 4] TEST: [2 5]\n",
      "TRAIN: [0 2 3 5] TEST: [1 4]\n",
      "[[1 2]\n",
      " [1 2]\n",
      " [3 4]\n",
      " [7 8]]\n",
      "[[3 4]\n",
      " [5 6]]\n",
      "[0 0 1 1]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "#K折数据切分\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6],[7, 8]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1])#1的个数和0的个数要大于3，3也就是n_splits\n",
    "skf = StratifiedKFold(n_splits=3,shuffle=True,random_state=1)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "\n",
    "X_train, X_test = X[train_index], X[test_index]\n",
    "y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "print(X_train)\n",
    "print(X_test)\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''模型融合中使用到的各个单模型'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "\n",
    "# 划分train数据集,调用代码,把数据集名字转成和代码一样\n",
    "X = np.array(train_data_features)\n",
    "y = np.array(label)\n",
    "\n",
    "X_test_features = np.array(test_data_features)\n",
    "\n",
    "stacking_LR = LR(penalty='l2', dual=True, random_state=0)\n",
    "\n",
    "stacking_xgb = XGBClassifier(n_estimators=150, min_samples_leaf=3, max_depth=6)\n",
    "\n",
    "stacking_rf = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=0)\n",
    "\n",
    "\n",
    "clfs = [stacking_LR,stacking_xgb,stacking_rf]#模型\n",
    "\n",
    "# 创建n_folds\n",
    "n_folds = 5\n",
    "skf = StratifiedKFold(n_splits=n_folds,shuffle=True,random_state=1)#K-Flod数据\n",
    "\n",
    "# 创建零矩阵（存储第一层的预测结果）\n",
    "dataset_blend_train = np.zeros((X.shape[0], len(clfs)))#行数：训练集的行数，列数：模型的个数\n",
    "\n",
    "dataset_blend_test = np.zeros((X_test_features.shape[0], len(clfs)))#行数：测试集数量，列数：模型的个数\n",
    "dataset_blend_test_j = np.zeros((X_test_features.shape[0], n_folds))#行数：测试集数量，列数：K折\n",
    "\n",
    "# 建立第一层模型\n",
    "for j, clf in enumerate(clfs):#枚举分类器\n",
    "    i = 0\n",
    "    for train_index, test_index in skf.split(X, y):#K折数据\n",
    "        X_1_train, y_1_train, X_1_test, y_1_test = X[train_index], y[train_index], X[test_index], y[test_index]\n",
    "        \n",
    "        clf.fit(X_1_train, y_1_train)#第j个模型预测第k折数据\n",
    "        \n",
    "        y_submission = clf.predict_proba(X_1_test)[:, 1]#第j个模型预测剩下的1折数据，去除答案是1的概率列\n",
    "        \n",
    "        dataset_blend_train[test_index, j] = y_submission#第j个模型预测的第k折数据的答案写到预测结果里\n",
    "        \n",
    "        dataset_blend_test_j[:, i] = clf.predict_proba(X_test_features)[:, 1]#对测试集进行预测\n",
    "        \n",
    "        i = i + 1 #第i折\n",
    "        \n",
    "    '''对于测试集，直接用这k个模型的预测值均值作为新的特征。'''\n",
    "    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1) #每个模型的K折的预测值取平均做为第j个分类器的预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "stacking auc 0.9506499199999999\n",
      "stacking auc 0.94645408\n",
      "stacking auc 0.9462804800000002\n",
      "stacking auc 0.9520553599999999\n",
      "stacking auc 0.9493892799999999\n",
      "0.9489658240000001\n",
      "0.1\n",
      "stacking auc 0.9507424000000001\n",
      "stacking auc 0.94657616\n",
      "stacking auc 0.9463376\n",
      "stacking auc 0.95219488\n",
      "stacking auc 0.9494544\n",
      "0.949061088\n",
      "1\n",
      "stacking auc 0.95079408\n",
      "stacking auc 0.9466462400000001\n",
      "stacking auc 0.9463564799999999\n",
      "stacking auc 0.9522491200000001\n",
      "stacking auc 0.94948288\n",
      "0.9491057599999999\n",
      "10\n",
      "stacking auc 0.9508049599999999\n",
      "stacking auc 0.94665712\n",
      "stacking auc 0.94635328\n",
      "stacking auc 0.9522579200000001\n",
      "stacking auc 0.94948464\n",
      "0.949111584\n"
     ]
    }
   ],
   "source": [
    "# 用建立第二层模型\n",
    "\n",
    "C = [0.01,0.1,1,10]\n",
    "\n",
    "for i in C:\n",
    "    stacking_model_lr = LR(C=i, max_iter=100)\n",
    "    print(i)\n",
    "    aucs = []\n",
    "    for train_index, test_index in skf.split(dataset_blend_train, y):#K折数据\n",
    "        X_2_train, y_2_train, X_2_test, y_2_test = dataset_blend_train[train_index], y[train_index], dataset_blend_train[test_index], y[test_index]\n",
    "        stacking_model_lr.fit(X_2_train, y_2_train)\n",
    "        test_predict_proba = stacking_model_lr.predict_proba(X_2_test)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_2_test, test_predict_proba, pos_label=1)\n",
    "        print(\"stacking auc\",auc(fpr, tpr))\n",
    "        aucs.append(auc(fpr, tpr))\n",
    "    print(np.average(aucs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "#stacking 预测\n",
    "stacking_model_lr = LR(C=10, max_iter=100)\n",
    "\n",
    "stacking_model_lr.fit(dataset_blend_train, y)\n",
    "\n",
    "test_predict = stacking_model_lr.predict(dataset_blend_test)\n",
    "\n",
    "test_predicted = np.array(test_predict)\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_stacking_wv.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、深度学习建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101245 unique tokens.\n",
      "Shape of data tensor: (50000, 100)\n",
      "Shape of label tensor: (25000, 2)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 100)          10124600  \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 10,205,202\n",
      "Trainable params: 10,205,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "train docs: 15999\n",
      "val docs: 4001\n",
      "test docs: 5000\n",
      "Train on 15999 samples, validate on 4001 samples\n",
      "Epoch 1/12\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.6922 - acc: 0.5401 - val_loss: 0.6898 - val_acc: 0.6151\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61510, saving model to lstm_model/lstm_weights-improvement-01-0.62.hdf5\n",
      "Epoch 2/12\n",
      "15999/15999 [==============================] - 40s 2ms/step - loss: 0.6865 - acc: 0.6664 - val_loss: 0.6823 - val_acc: 0.6776\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61510 to 0.67758, saving model to lstm_model/lstm_weights-improvement-02-0.68.hdf5\n",
      "Epoch 3/12\n",
      "15999/15999 [==============================] - 38s 2ms/step - loss: 0.6728 - acc: 0.7438 - val_loss: 0.6590 - val_acc: 0.7423\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.67758 to 0.74231, saving model to lstm_model/lstm_weights-improvement-03-0.74.hdf5\n",
      "Epoch 4/12\n",
      "15999/15999 [==============================] - 47s 3ms/step - loss: 0.6261 - acc: 0.8066 - val_loss: 0.5257 - val_acc: 0.7908\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.74231 to 0.79080, saving model to lstm_model/lstm_weights-improvement-04-0.79.hdf5\n",
      "Epoch 5/12\n",
      "15999/15999 [==============================] - 46s 3ms/step - loss: 0.5113 - acc: 0.7992 - val_loss: 0.4856 - val_acc: 0.8180\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.79080 to 0.81805, saving model to lstm_model/lstm_weights-improvement-05-0.82.hdf5\n",
      "Epoch 6/12\n",
      "15999/15999 [==============================] - 40s 2ms/step - loss: 0.4427 - acc: 0.8469 - val_loss: 0.3895 - val_acc: 0.8460\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81805 to 0.84604, saving model to lstm_model/lstm_weights-improvement-06-0.85.hdf5\n",
      "Epoch 7/12\n",
      "15999/15999 [==============================] - 44s 3ms/step - loss: 0.3473 - acc: 0.8820 - val_loss: 0.3471 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.84604 to 0.85829, saving model to lstm_model/lstm_weights-improvement-07-0.86.hdf5\n",
      "Epoch 8/12\n",
      "15999/15999 [==============================] - 46s 3ms/step - loss: 0.2877 - acc: 0.9022 - val_loss: 0.3206 - val_acc: 0.8670\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.85829 to 0.86703, saving model to lstm_model/lstm_weights-improvement-08-0.87.hdf5\n",
      "Epoch 9/12\n",
      "15999/15999 [==============================] - 46s 3ms/step - loss: 0.2308 - acc: 0.9202 - val_loss: 0.3152 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.86703 to 0.86928, saving model to lstm_model/lstm_weights-improvement-09-0.87.hdf5\n",
      "Epoch 10/12\n",
      "15999/15999 [==============================] - 46s 3ms/step - loss: 0.1895 - acc: 0.9388 - val_loss: 0.3132 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86928 to 0.87353, saving model to lstm_model/lstm_weights-improvement-10-0.87.hdf5\n",
      "Epoch 11/12\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.1540 - acc: 0.9499 - val_loss: 0.3206 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87353\n",
      "Epoch 12/12\n",
      "15999/15999 [==============================] - 38s 2ms/step - loss: 0.1284 - acc: 0.9604 - val_loss: 0.3267 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.87353 to 0.87503, saving model to lstm_model/lstm_weights-improvement-12-0.88.hdf5\n",
      "5000/5000 [==============================] - 2s 496us/step\n",
      "[0.3418441625118256, 0.866]\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100 # 每条新闻最大长度\n",
    "EMBEDDING_DIM = 100       # 词向量空间维度\n",
    "\n",
    "all_data = train_data+test_data\n",
    "#Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)\n",
    "\n",
    "#总共词数(word_index：key:词，value:索引)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(word_index)\n",
    "\n",
    "#将整篇文章根据向量化文本序列都退少补生成文章矩阵\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train,x_test = data[:len(train_data)],data[len(train_data):]\n",
    "\n",
    "#将标签独热向量处理\n",
    "labels = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))#LSTM参数：LSTM的输出向量的维度\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例\n",
    "\n",
    "p1 = int(len(x_train)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(x_train)*(1-TEST_SPLIT))\n",
    "\n",
    "train_x = x_train[:p1]\n",
    "train_y = labels[:p1]\n",
    "val_x = x_train[p1:p2]\n",
    "val_y = labels[p1:p2]\n",
    "test_x = x_train[p2:]\n",
    "test_y = labels[p2:]\n",
    "\n",
    "print ('train docs: '+str(len(train_x)))\n",
    "print ('val docs: '+str(len(val_x)))\n",
    "print ('test docs: '+str(len(test_x)))\n",
    "\n",
    "filepath=\"lstm_model/lstm_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=12, batch_size=5000,callbacks=callbacks_list)\n",
    "\n",
    "#model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(test_x, test_y))\n",
    "\n",
    "test_predicted = np.array(model.predict_classes(x_test))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_lstm.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101245 unique tokens.\n",
      "Shape of data tensor: (50000, 100)\n",
      "Shape of label tensor: (25000, 2)\n",
      "train docs: 15999\n",
      "val docs: 4001\n",
      "test docs: 5000\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          10124600  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 250)           75250     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               800100    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 11,000,152\n",
      "Trainable params: 11,000,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 15999 samples, validate on 4001 samples\n",
      "Epoch 1/6\n",
      "15000/15999 [===========================>..] - ETA: 21s - loss: 1.0420 - acc: 0.5028 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (5.411809). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15999/15999 [==============================] - 360s 22ms/step - loss: 1.0201 - acc: 0.5049 - val_loss: 0.6928 - val_acc: 0.4989\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.49888, saving model to weights-improvement-01-0.50.hdf5\n",
      "Epoch 2/6\n",
      "15999/15999 [==============================] - 125s 8ms/step - loss: 0.6877 - acc: 0.5836 - val_loss: 0.6910 - val_acc: 0.4986\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.49888\n",
      "Epoch 3/6\n",
      "15999/15999 [==============================] - 36s 2ms/step - loss: 0.6810 - acc: 0.5130 - val_loss: 0.6879 - val_acc: 0.4986\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.49888\n",
      "Epoch 4/6\n",
      "15999/15999 [==============================] - 33s 2ms/step - loss: 0.6656 - acc: 0.5419 - val_loss: 0.6849 - val_acc: 0.4994\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.49888 to 0.49938, saving model to weights-improvement-04-0.50.hdf5\n",
      "Epoch 5/6\n",
      "15999/15999 [==============================] - 56s 4ms/step - loss: 0.6498 - acc: 0.5272 - val_loss: 0.6014 - val_acc: 0.7183\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.49938 to 0.71832, saving model to weights-improvement-05-0.72.hdf5\n",
      "Epoch 6/6\n",
      "15999/15999 [==============================] - 34s 2ms/step - loss: 0.5499 - acc: 0.7414 - val_loss: 0.5173 - val_acc: 0.7471\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.71832 to 0.74706, saving model to weights-improvement-06-0.75.hdf5\n",
      "5000/5000 [==============================] - 4s 865us/step\n",
      "[0.5218041631698609, 0.7492]\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          1\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100 # 每条新闻最大长度\n",
    "EMBEDDING_DIM = 100 # 词向量空间维度\n",
    "\n",
    "\n",
    "\n",
    "#合并训练集和测试集\n",
    "all_data = train_data+test_data\n",
    "\n",
    "#Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)\n",
    "\n",
    "#总共词数(word_index：key:词，value:词频)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(word_index)\n",
    "\n",
    "#将整篇文章根据向量化文本序列多退少补生成文章矩阵\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train,x_test = data[:len(train_data)],data[len(train_data):]\n",
    "\n",
    "#将标签独热向量处理\n",
    "labels = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例\n",
    "\n",
    "p1 = int(len(x_train)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(x_train)*(1-TEST_SPLIT))\n",
    "\n",
    "train_x = x_train[:p1]\n",
    "train_y = labels[:p1]\n",
    "val_x = x_train[p1:p2]\n",
    "val_y = labels[p1:p2]\n",
    "test_x = x_train[p2:]\n",
    "test_y = labels[p2:]\n",
    "\n",
    "print ('train docs: '+str(len(train_x)))\n",
    "print ('val docs: '+str(len(val_x)))\n",
    "print ('test docs: '+str(len(test_x)))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "filepath=\"cnn_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=12, batch_size=5000,callbacks=callbacks_list)\n",
    "\n",
    "#model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(test_x, test_y))\n",
    "\n",
    "test_predicted = np.array(model.predict_classes(x_test))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_cnn.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Word2vec+深度学习建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.LSTM+Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101245 unique tokens.\n",
      "Shape of data tensor: (50000, 100)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:39: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:40: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          10124600  \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 10,205,202\n",
      "Trainable params: 80,602\n",
      "Non-trainable params: 10,124,600\n",
      "_________________________________________________________________\n",
      "train docs: 15999\n",
      "val docs: 4001\n",
      "test docs: 5000\n",
      "Train on 15999 samples, validate on 4001 samples\n",
      "Epoch 1/40\n",
      "15999/15999 [==============================] - 36s 2ms/step - loss: 0.6852 - acc: 0.5797 - val_loss: 0.6684 - val_acc: 0.7096\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.70957, saving model to lstm_word2vec_weights-improvement-01-0.71.hdf5\n",
      "Epoch 2/40\n",
      "15999/15999 [==============================] - 27s 2ms/step - loss: 0.6619 - acc: 0.6885 - val_loss: 0.6328 - val_acc: 0.7546\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.70957 to 0.75456, saving model to lstm_word2vec_weights-improvement-02-0.75.hdf5\n",
      "Epoch 3/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.6155 - acc: 0.7386 - val_loss: 0.5217 - val_acc: 0.7928\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.75456 to 0.79280, saving model to lstm_word2vec_weights-improvement-03-0.79.hdf5\n",
      "Epoch 4/40\n",
      "15999/15999 [==============================] - 30s 2ms/step - loss: 0.4992 - acc: 0.7850 - val_loss: 0.4086 - val_acc: 0.8230\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.79280 to 0.82304, saving model to lstm_word2vec_weights-improvement-04-0.82.hdf5\n",
      "Epoch 5/40\n",
      "15999/15999 [==============================] - 30s 2ms/step - loss: 0.4629 - acc: 0.8022 - val_loss: 0.4273 - val_acc: 0.8295\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.82304 to 0.82954, saving model to lstm_word2vec_weights-improvement-05-0.83.hdf5\n",
      "Epoch 6/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.4421 - acc: 0.8135 - val_loss: 0.3680 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.82954 to 0.85154, saving model to lstm_word2vec_weights-improvement-06-0.85.hdf5\n",
      "Epoch 7/40\n",
      "15999/15999 [==============================] - 30s 2ms/step - loss: 0.4188 - acc: 0.8216 - val_loss: 0.3469 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.85154 to 0.85754, saving model to lstm_word2vec_weights-improvement-07-0.86.hdf5\n",
      "Epoch 8/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.4013 - acc: 0.8312 - val_loss: 0.3650 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.85754\n",
      "Epoch 9/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.3988 - acc: 0.8309 - val_loss: 0.3352 - val_acc: 0.8628\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.85754 to 0.86278, saving model to lstm_word2vec_weights-improvement-09-0.86.hdf5\n",
      "Epoch 10/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.3877 - acc: 0.8385 - val_loss: 0.3276 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86278 to 0.86603, saving model to lstm_word2vec_weights-improvement-10-0.87.hdf5\n",
      "Epoch 11/40\n",
      "15999/15999 [==============================] - 29s 2ms/step - loss: 0.3876 - acc: 0.8387 - val_loss: 0.3492 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86603\n",
      "Epoch 12/40\n",
      "15999/15999 [==============================] - 32s 2ms/step - loss: 0.3873 - acc: 0.8377 - val_loss: 0.3324 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86603\n",
      "Epoch 13/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3783 - acc: 0.8371 - val_loss: 0.3253 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86603\n",
      "Epoch 14/40\n",
      "15999/15999 [==============================] - 40s 2ms/step - loss: 0.3739 - acc: 0.8438 - val_loss: 0.3292 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86603\n",
      "Epoch 15/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3735 - acc: 0.8406 - val_loss: 0.3234 - val_acc: 0.8665\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.86603 to 0.86653, saving model to lstm_word2vec_weights-improvement-15-0.87.hdf5\n",
      "Epoch 16/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3714 - acc: 0.8422 - val_loss: 0.3225 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.86653 to 0.86678, saving model to lstm_word2vec_weights-improvement-16-0.87.hdf5\n",
      "Epoch 17/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3662 - acc: 0.8457 - val_loss: 0.3213 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.86678 to 0.86728, saving model to lstm_word2vec_weights-improvement-17-0.87.hdf5\n",
      "Epoch 18/40\n",
      "15999/15999 [==============================] - 38s 2ms/step - loss: 0.3721 - acc: 0.8429 - val_loss: 0.3255 - val_acc: 0.8645\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86728\n",
      "Epoch 19/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3736 - acc: 0.8409 - val_loss: 0.3223 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86728\n",
      "Epoch 20/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3659 - acc: 0.8434 - val_loss: 0.3215 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86728\n",
      "Epoch 21/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3648 - acc: 0.8464 - val_loss: 0.3167 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.86728 to 0.86928, saving model to lstm_word2vec_weights-improvement-21-0.87.hdf5\n",
      "Epoch 22/40\n",
      "15999/15999 [==============================] - 33s 2ms/step - loss: 0.3711 - acc: 0.8407 - val_loss: 0.3228 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.86928\n",
      "Epoch 23/40\n",
      "15999/15999 [==============================] - 32s 2ms/step - loss: 0.3659 - acc: 0.8459 - val_loss: 0.3155 - val_acc: 0.8685\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.86928\n",
      "Epoch 24/40\n",
      "15999/15999 [==============================] - 37s 2ms/step - loss: 0.3635 - acc: 0.8479 - val_loss: 0.3183 - val_acc: 0.8673\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.86928\n",
      "Epoch 25/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3599 - acc: 0.8458 - val_loss: 0.3164 - val_acc: 0.8698\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.86928 to 0.86978, saving model to lstm_word2vec_weights-improvement-25-0.87.hdf5\n",
      "Epoch 26/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3630 - acc: 0.8477 - val_loss: 0.3216 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.86978\n",
      "Epoch 27/40\n",
      "15999/15999 [==============================] - 40s 2ms/step - loss: 0.3627 - acc: 0.8472 - val_loss: 0.3142 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00027: val_acc improved from 0.86978 to 0.87153, saving model to lstm_word2vec_weights-improvement-27-0.87.hdf5\n",
      "Epoch 28/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3618 - acc: 0.8476 - val_loss: 0.3188 - val_acc: 0.8660\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.87153\n",
      "Epoch 29/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3612 - acc: 0.8477 - val_loss: 0.3150 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.87153\n",
      "Epoch 30/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3506 - acc: 0.8524 - val_loss: 0.3130 - val_acc: 0.8715\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.87153\n",
      "Epoch 31/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3590 - acc: 0.8481 - val_loss: 0.3235 - val_acc: 0.8640\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.87153\n",
      "Epoch 32/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3596 - acc: 0.8474 - val_loss: 0.3114 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00032: val_acc improved from 0.87153 to 0.87178, saving model to lstm_word2vec_weights-improvement-32-0.87.hdf5\n",
      "Epoch 33/40\n",
      "15999/15999 [==============================] - 38s 2ms/step - loss: 0.3595 - acc: 0.8480 - val_loss: 0.3313 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.87178\n",
      "Epoch 34/40\n",
      "15999/15999 [==============================] - 34s 2ms/step - loss: 0.3598 - acc: 0.8461 - val_loss: 0.3076 - val_acc: 0.8733\n",
      "\n",
      "Epoch 00034: val_acc improved from 0.87178 to 0.87328, saving model to lstm_word2vec_weights-improvement-34-0.87.hdf5\n",
      "Epoch 35/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3564 - acc: 0.8486 - val_loss: 0.3217 - val_acc: 0.8643\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.87328\n",
      "Epoch 36/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3522 - acc: 0.8522 - val_loss: 0.3051 - val_acc: 0.8740\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.87328 to 0.87403, saving model to lstm_word2vec_weights-improvement-36-0.87.hdf5\n",
      "Epoch 37/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3558 - acc: 0.8502 - val_loss: 0.3213 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.87403\n",
      "Epoch 38/40\n",
      "15999/15999 [==============================] - 36s 2ms/step - loss: 0.3533 - acc: 0.8494 - val_loss: 0.3094 - val_acc: 0.8713\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.87403\n",
      "Epoch 39/40\n",
      "15999/15999 [==============================] - 35s 2ms/step - loss: 0.3554 - acc: 0.8499 - val_loss: 0.3129 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.87403\n",
      "Epoch 40/40\n",
      "15999/15999 [==============================] - 39s 2ms/step - loss: 0.3502 - acc: 0.8516 - val_loss: 0.3079 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.87403\n",
      "5000/5000 [==============================] - 3s 636us/step\n",
      "[0.31709566490650176, 0.868]\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "all_data = train_data+test_data\n",
    "#Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)\n",
    "\n",
    "#总共词数(word_index：key:词，value:索引)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(word_index)\n",
    "\n",
    "#将整篇文章根据向量化文本序列都退少补生成文章矩阵\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train,x_test = data[:len(train_data)],data[len(train_data):]\n",
    "\n",
    "#将标签独热向量处理\n",
    "labels = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "wv_model = Word2Vec.load(\"100size_3min_count_10window.model\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    if word in wv_model:\n",
    "        embedding_matrix[i] = np.asarray(wv_model[word],dtype='float32')\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,             #input_dim：词向量矩阵的维度\n",
    "                            EMBEDDING_DIM,                   #output_dim:词向量的长度\n",
    "                            weights=[embedding_matrix],      #weights：词向量矩阵\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,#input_length：句子的最大长度\n",
    "                            trainable=False)                 #trainable：是否冻结嵌入层      \n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))#LSTM参数：LSTM的输出向量的维度\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例\n",
    "\n",
    "p1 = int(len(x_train)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(x_train)*(1-TEST_SPLIT))\n",
    "\n",
    "train_x = x_train[:p1]\n",
    "train_y = labels[:p1]\n",
    "val_x = x_train[p1:p2]\n",
    "val_y = labels[p1:p2]\n",
    "test_x = x_train[p2:]\n",
    "test_y = labels[p2:]\n",
    "\n",
    "print ('train docs: '+str(len(train_x)))\n",
    "print ('val docs: '+str(len(val_x)))\n",
    "print ('test docs: '+str(len(test_x)))\n",
    "\n",
    "filepath=\"lstm_word2vec_model/lstm_word2vec_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "#仅保存最好的模型\n",
    "#filepath=\"weights.best.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')#验证集准确率比之前效果好就保存权重\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=40, batch_size=5000,callbacks=callbacks_list)\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "\n",
    "#model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(test_x, test_y))\n",
    "\n",
    "\n",
    "test_predicted = np.array(model.predict_classes(x_test))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_lstm_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载模型（使用保存的模型评估或继续训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 3s 519us/step\n",
      "[0.31665992724895475, 0.8666]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))#LSTM参数：LSTM的输出向量的维度\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# load weights\n",
    "model.load_weights(\"lstm_word2vec_weights-improvement-36-0.87.hdf5\")\n",
    "\n",
    "#model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(test_x, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.CNN+Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101245 unique tokens.\n",
      "Shape of data tensor: (50000, 100)\n",
      "Shape of label tensor: (25000, 2)\n",
      "train docs: 15999\n",
      "val docs: 4001\n",
      "test docs: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:61: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:62: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 100)          10124600  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 98, 250)           75250     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 32, 250)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               800100    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 11,000,152\n",
      "Trainable params: 875,552\n",
      "Non-trainable params: 10,124,600\n",
      "_________________________________________________________________\n",
      "Train on 15999 samples, validate on 4001 samples\n",
      "Epoch 1/20\n",
      "15999/15999 [==============================] - 18s 1ms/step - loss: 1.8571 - acc: 0.5046 - val_loss: 0.6610 - val_acc: 0.6353\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.63534, saving model to cnn_model/cnn_word2vec_weights-improvement-01-0.64.hdf5\n",
      "Epoch 2/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.6381 - acc: 0.6811 - val_loss: 0.6190 - val_acc: 0.6473\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.63534 to 0.64734, saving model to cnn_model/cnn_word2vec_weights-improvement-02-0.65.hdf5\n",
      "Epoch 3/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.6863 - acc: 0.5880 - val_loss: 0.5875 - val_acc: 0.6901\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.64734 to 0.69008, saving model to cnn_model/cnn_word2vec_weights-improvement-03-0.69.hdf5\n",
      "Epoch 4/20\n",
      "15999/15999 [==============================] - 20s 1ms/step - loss: 0.5728 - acc: 0.6988 - val_loss: 0.5574 - val_acc: 0.7083\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.69008 to 0.70832, saving model to cnn_model/cnn_word2vec_weights-improvement-04-0.71.hdf5\n",
      "Epoch 5/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.5576 - acc: 0.7026 - val_loss: 0.5020 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.70832 to 0.75381, saving model to cnn_model/cnn_word2vec_weights-improvement-05-0.75.hdf5\n",
      "Epoch 6/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.6535 - acc: 0.6495 - val_loss: 0.4529 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.75381 to 0.82679, saving model to cnn_model/cnn_word2vec_weights-improvement-06-0.83.hdf5\n",
      "Epoch 7/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.4425 - acc: 0.8086 - val_loss: 0.5880 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.82679\n",
      "Epoch 8/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.5308 - acc: 0.7394 - val_loss: 0.3934 - val_acc: 0.8383\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.82679 to 0.83829, saving model to cnn_model/cnn_word2vec_weights-improvement-08-0.84.hdf5\n",
      "Epoch 9/20\n",
      "15999/15999 [==============================] - 19s 1ms/step - loss: 0.4318 - acc: 0.8035 - val_loss: 0.4658 - val_acc: 0.7731\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.83829\n",
      "Epoch 10/20\n",
      "15999/15999 [==============================] - 17s 1ms/step - loss: 0.4140 - acc: 0.8127 - val_loss: 0.3626 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.83829 to 0.84229, saving model to cnn_model/cnn_word2vec_weights-improvement-10-0.84.hdf5\n",
      "Epoch 11/20\n",
      "15999/15999 [==============================] - 16s 990us/step - loss: 0.3769 - acc: 0.8356 - val_loss: 0.4600 - val_acc: 0.7823\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.84229\n",
      "Epoch 12/20\n",
      "15999/15999 [==============================] - 15s 943us/step - loss: 0.4385 - acc: 0.7902 - val_loss: 0.3469 - val_acc: 0.8540\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.84229 to 0.85404, saving model to cnn_model/cnn_word2vec_weights-improvement-12-0.85.hdf5\n",
      "Epoch 13/20\n",
      "15999/15999 [==============================] - 15s 944us/step - loss: 0.3454 - acc: 0.8516 - val_loss: 0.3733 - val_acc: 0.8293\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.85404\n",
      "Epoch 14/20\n",
      "15999/15999 [==============================] - 15s 944us/step - loss: 0.4367 - acc: 0.7958 - val_loss: 0.3613 - val_acc: 0.8423\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.85404\n",
      "Epoch 15/20\n",
      "15999/15999 [==============================] - 15s 941us/step - loss: 0.3444 - acc: 0.8539 - val_loss: 0.3234 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.85404 to 0.85979, saving model to cnn_model/cnn_word2vec_weights-improvement-15-0.86.hdf5\n",
      "Epoch 16/20\n",
      "15999/15999 [==============================] - 15s 944us/step - loss: 0.3219 - acc: 0.8628 - val_loss: 0.3491 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.85979\n",
      "Epoch 17/20\n",
      "15999/15999 [==============================] - 15s 946us/step - loss: 0.4664 - acc: 0.7804 - val_loss: 0.3365 - val_acc: 0.8595\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.85979\n",
      "Epoch 18/20\n",
      "15999/15999 [==============================] - 15s 953us/step - loss: 0.3263 - acc: 0.8640 - val_loss: 0.3366 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.85979\n",
      "Epoch 19/20\n",
      "15999/15999 [==============================] - 15s 940us/step - loss: 0.3643 - acc: 0.8394 - val_loss: 0.3416 - val_acc: 0.8503\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.85979\n",
      "Epoch 20/20\n",
      "15999/15999 [==============================] - 15s 956us/step - loss: 0.3400 - acc: 0.8516 - val_loss: 0.3439 - val_acc: 0.8485\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.85979\n",
      "5000/5000 [==============================] - 2s 404us/step\n",
      "[0.35134012341499327, 0.8474]\n",
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          1\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100 # 每条新闻最大长度\n",
    "EMBEDDING_DIM = 100       # 词向量空间维度\n",
    "\n",
    "VALIDATION_SPLIT = 0.16 # 验证集比例\n",
    "TEST_SPLIT = 0.2 # 测试集比例\n",
    "\n",
    "#合并训练集和测试集\n",
    "all_data = train_data+test_data\n",
    "\n",
    "#Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_data)\n",
    "sequences = tokenizer.texts_to_sequences(all_data)\n",
    "\n",
    "#总共词数(word_index：key:词，value:索引)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "#print(word_index)\n",
    "\n",
    "#将整篇文章根据向量化文本序列都退少补生成文章矩阵\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "\n",
    "x_train,x_test = data[:len(train_data)],data[len(train_data):]\n",
    "\n",
    "#将标签独热向量处理\n",
    "labels = to_categorical(np.asarray(label))\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "\n",
    "p1 = int(len(x_train)*(1-VALIDATION_SPLIT-TEST_SPLIT))\n",
    "p2 = int(len(x_train)*(1-TEST_SPLIT))\n",
    "\n",
    "train_x = x_train[:p1]\n",
    "train_y = labels[:p1]\n",
    "val_x = x_train[p1:p2]\n",
    "val_y = labels[p1:p2]\n",
    "test_x = x_train[p2:]\n",
    "test_y = labels[p2:]\n",
    "\n",
    "print ('train docs: '+str(len(train_x)))\n",
    "print ('val docs: '+str(len(val_x)))\n",
    "print ('test docs: '+str(len(test_x)))\n",
    "\n",
    "\n",
    "wv_model = Word2Vec.load(\"100size_3min_count_10window.model\")\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items(): \n",
    "    if word in wv_model:\n",
    "        embedding_matrix[i] = np.asarray(wv_model[word],dtype='float32')\n",
    "        \n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)#冻结嵌入层\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(250, 3, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(EMBEDDING_DIM, activation='relu'))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "#plot_model(model, to_file='model.png',show_shapes=True)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "filepath=\"cnn_model/cnn_word2vec_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True,mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X, Y, validation_split=0.33, nb_epoch=150, batch_size=10,callbacks=callbacks_list, verbose=0)\n",
    "\n",
    "\n",
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=20, batch_size=5000,callbacks=callbacks_list)\n",
    "\n",
    "#model.save('word_vector_cnn.h5')\n",
    "print (model.evaluate(test_x, test_y))\n",
    "\n",
    "test_predicted = np.array(model.predict_classes(x_test))\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_cnn_word2vec.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSM + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review\n",
       "0          1  With all this stuff going down at the moment w...\n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2          0  The film starts with a manager (Nicholas Bell)...\n",
       "3          0  It must be assumed that those who praised this...\n",
       "4          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import sys\n",
    "from imp import reload\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "if sys.version[0] == '2':\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding(\"utf-8\")\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv('data/labeledTrainData.tsv', delimiter=\"\\t\")\n",
    "df1 = df1.drop(['id'], axis=1)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('data/imdb_master.csv',encoding=\"latin-1\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  Once again Mr. Costner has dragged out a movie...       neg\n",
       "1  This is an example of why the majority of acti...       neg\n",
       "2  First of all I hate those moronic rappers, who...       neg\n",
       "3  Not even the Beatles could write songs everyon...       neg\n",
       "4  Brass pictures (movies is not a fitting word f...       neg"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.drop(['Unnamed: 0','type','file'],axis=1)\n",
    "df2.columns = [\"review\",\"sentiment\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Once again Mr. Costner has dragged out a movie...          0\n",
       "1  This is an example of why the majority of acti...          0\n",
       "2  First of all I hate those moronic rappers, who...          0\n",
       "3  Not even the Beatles could write songs everyon...          0\n",
       "4  Brass pictures (movies is not a fitting word f...          0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2.sentiment != 'unsup']\n",
    "df2['sentiment'] = df2['sentiment'].map({'pos': 1, 'neg': 0})\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>Processed_Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>stuff go moment mj ive start listen music watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>classic war world timothy hines entertain film...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>film start manager nicholas bell give welcome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>must assume praise film greatest film opera ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy wondrously unpretentious 80 ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                             review  \\\n",
       "0          1  With all this stuff going down at the moment w...   \n",
       "1          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3          0  It must be assumed that those who praised this...   \n",
       "4          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                   Processed_Reviews  \n",
       "0  stuff go moment mj ive start listen music watc...  \n",
       "1  classic war world timothy hines entertain film...  \n",
       "2  film start manager nicholas bell give welcome ...  \n",
       "3  must assume praise film greatest film opera ev...  \n",
       "4  superbly trashy wondrously unpretentious 80 ex...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1#pd.concat([df1, df2])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n",
    "    text = text.lower()\n",
    "    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n",
    "    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n",
    "    text = [word for word in text if not word in stop_words]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "df['Processed_Reviews'] = df.review.apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129.54916"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Processed_Reviews.apply(lambda x: len(x.split(\" \"))).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "max_features = 6000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(df['Processed_Reviews'])#词序列化\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df['Processed_Reviews'])#生成文章序列\n",
    "\n",
    "maxlen = 130\n",
    "#将整篇文章根据向量化文本序列都退少补生成文章矩阵\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "y = df['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 128)         768000    \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          41216     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 810,537\n",
      "Trainable params: 810,537\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_size))\n",
    "model.add(Bidirectional(LSTM(32, return_sequences = True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(20, activation=\"relu\"))\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 49s 2ms/step - loss: 0.4530 - acc: 0.7857 - val_loss: 0.3215 - val_acc: 0.8672\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 47s 2ms/step - loss: 0.2471 - acc: 0.9030 - val_loss: 0.3053 - val_acc: 0.8702\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 48s 2ms/step - loss: 0.1829 - acc: 0.9311 - val_loss: 0.3511 - val_acc: 0.8638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3d6c4b38>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.8606749612433915\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10669,  1674],\n",
       "       [ 1831, 10826]], dtype=int64)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_csv(\"data/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_test.head()\n",
    "df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\n",
    "df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = df_test[\"sentiment\"]\n",
    "list_sentences_test = df_test[\"review\"]\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df_test=pd.read_csv(\"data/testData.tsv\",header=0, delimiter=\"\\t\", quoting=1)\n",
    "test_predicted = model.predict_classes(X_te).reshape(1,df_test.shape[0])[0]\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': df_test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_lstm_imdb_master.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras import backend as K#返回当前后端\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Embedding,LSTM,Layer,initializers,regularizers,constraints,Input,Dropout,concatenate,BatchNormalization\n",
    "from keras.layers import Dense,Bidirectional,Concatenate,Multiply,Maximum,Subtract,Lambda,dot,Flatten,Reshape\n",
    "import gc\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "#静态Attention Model\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self,step_dim,W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\t#正则化\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\t\t#约束、限制\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\t\t\t\t\t\t\t\t\t\t#偏置\n",
    "        self.step_dim = step_dim\t\t\t\t\t\t\t\t#step维度\n",
    "        self.features_dim = 0\t\t\t\t\t\t\t\t\t#特征维度\n",
    "\n",
    "        super(AttentionLayer,self).__init__(**kwargs)#用于调用父类(超类)的一个方法。\n",
    "\n",
    "    #设置self.supports_masking = True后需要复写该方法\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return None\n",
    "\n",
    "    #参数设置，必须实现\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],),#词向量维度，神经元个数\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]#词向量维度\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),#词的个数，神经元个数\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,#正则化\n",
    "                                     constraint=self.b_constraint)\t#约束和限制\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    # input (None,sentence_length,embedding_size)\n",
    "    def call(self, x, mask = None):\n",
    "        # 计算输出\n",
    "        features_dim = self.features_dim#词向量维度\n",
    "        step_dim = self.step_dim\t\t#词的个数\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        # print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n",
    "\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'step_dim': self.step_dim}\n",
    "        base_config = super(AttentionLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 130)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 130, 128)     768000      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer_3 (AttentionLay (None, 128)          258         embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 64)           41216       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 192)          0           attention_layer_3[0][0]          \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 20)           3860        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 20)           0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            21          dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 813,355\n",
      "Trainable params: 813,355\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_size = 128\n",
    "\n",
    "left_input = Input(shape=(130,), dtype='int32')\n",
    "# 定义需要使用的网络层\n",
    "embedding_layer1 = Embedding(\n",
    "    input_dim=6000,\n",
    "    output_dim=128,\n",
    "    trainable=True,\n",
    "    input_length=130\n",
    ")\n",
    "att_layer1 = AttentionLayer(130)\n",
    "\n",
    "#bi_lstm_layer =Bidirectional(LSTM(32, return_sequences = True))\n",
    "bi_lstm_layer =Bidirectional(LSTM(32))\n",
    "\n",
    "#bi_lstm_layer = globalMaxp(bi_lstm_layer)\n",
    "\n",
    "s1 = embedding_layer1(left_input)\n",
    "s1_bi = bi_lstm_layer(s1)\n",
    "s1_att = att_layer1(s1)\n",
    "s1_last = Concatenate(axis=1)([s1_att,s1_bi])#横着拼接\n",
    "dense_layer1 = Dense(20,activation='relu')(s1_last)\n",
    "dropout1 = Dropout(0.05)(dense_layer1)\n",
    "dense_layer2 = Dense(1,activation='sigmoid')(dropout1)\n",
    "\n",
    "model = Model(inputs=left_input,outputs=[dense_layer2], name=\"simaese_lstm_attention\")\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 45s 2ms/step - loss: 0.0522 - acc: 0.9840 - val_loss: 0.5606 - val_acc: 0.8504\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 45s 2ms/step - loss: 0.0473 - acc: 0.9850 - val_loss: 0.6668 - val_acc: 0.8500\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 44s 2ms/step - loss: 0.0272 - acc: 0.9921 - val_loss: 0.7165 - val_acc: 0.8512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x44882518>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "epochs = 3\n",
    "model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.842258181671442\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10655,  2064],\n",
       "       [ 1845, 10436]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test=pd.read_csv(\"data/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)\n",
    "df_test.head()\n",
    "df_test[\"review\"]=df_test.review.apply(lambda x: clean_text(x))\n",
    "df_test[\"sentiment\"] = df_test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = df_test[\"sentiment\"]\n",
    "list_sentences_test = df_test[\"review\"]\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "prediction = model.predict(X_te)\n",
    "y_pred = (prediction > 0.5)\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "print('F1-score: {0}'.format(f1_score(y_pred, y_test)))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存结果...\n",
      "         id  sentiment\n",
      "0  12311_10          1\n",
      "1    8348_2          0\n",
      "2    5828_4          0\n",
      "3    7186_2          0\n",
      "4   12128_7          1\n",
      "结束.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_predicted = np.array(model.predict(X_te))\n",
    "test_predicted =list(map(lambda x:1 if x >0.5 else 0 ,test_predicted))\n",
    "\n",
    "df_test=pd.read_csv(\"data/testData.tsv\",header=0, delimiter=\"\\t\", quoting=1)\n",
    "\n",
    "print('保存结果...')\n",
    "submission_df = pd.DataFrame(data ={'id': df_test['id'], 'sentiment': test_predicted})\n",
    "print(submission_df.head(5))\n",
    "submission_df.to_csv('submission_DSSM_bi-lstm_attention_imdb_master.csv',columns = ['id','sentiment'], index = False)\n",
    "print('结束.')\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 共578组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxPooling1D和GlobalMaxPooling1D的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 6, 16)             1728      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 5, 16)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 10)                1080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,819\n",
      "Trainable params: 2,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GlobalMaxPooling1D,MaxPooling1D\n",
    "\n",
    "D = np.random.rand(10, 6, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(6, 10), return_sequences=True))\n",
    "model.add(MaxPooling1D(pool_size=2, strides=1))\n",
    "model.add(LSTM(10))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "# print the summary to see how the dimension change after the layers are \n",
    "# applied\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 6, 16)             1728      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 1,745\n",
      "Trainable params: 1,745\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# try a model with GlobalMaxPooling1D now\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(6, 10), return_sequences=True))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
